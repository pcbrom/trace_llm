{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **First Step**\n",
    "## **Zero-Shot prompt engineering for Naive and JSON output for itens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('data/res_step_one_clean.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(output):\n",
    "    cot = re.search(r'\"CoT\":\\s*\"([^\"]*)\"', output)\n",
    "    answer = re.search(r'\"answer\":\\s*\"([^\"]*)\"', output)\n",
    "    justification = re.search(r'\"justification\":\\s*\"([^\"]*)\"', output)\n",
    "    alternative_answer = re.search(r'\"alternative_answer\":\\s*\"([^\"]*)\"', output)\n",
    "\n",
    "    cot_value = cot.group(1) if cot else None\n",
    "    answer_value = answer.group(1) if answer else None\n",
    "    justification_value = justification.group(1) if justification else None\n",
    "    alternative_answer_value = alternative_answer.group(1) if alternative_answer else None\n",
    "\n",
    "    # Count the number of steps in the CoT by counting the number of sentences (assuming sentences end with a period)\n",
    "    cot_steps = None\n",
    "    if cot_value:\n",
    "        # Count sentences by counting periods\n",
    "        cot_steps = len(re.findall(r'[0-9]{1}\\.', cot_value))\n",
    "\n",
    "    # Calculate extra_text by removing the matched spans\n",
    "    extra_text = output\n",
    "    if cot:\n",
    "        extra_text = extra_text.replace(cot.group(0), \"\")\n",
    "    if answer:\n",
    "        extra_text = extra_text.replace(answer.group(0), \"\")\n",
    "    if justification:\n",
    "        extra_text = extra_text.replace(justification.group(0), \"\")\n",
    "    if alternative_answer:\n",
    "        extra_text = extra_text.replace(alternative_answer.group(0), \"\")\n",
    "    \n",
    "    extra_text_value = re.sub(r'\\{.*?\\}', '', extra_text, flags=re.DOTALL).strip()\n",
    "    extra_text_value = extra_text_value.replace('```json\\n\\n```', '')\n",
    "\n",
    "    return cot_value, answer_value, alternative_answer_value, justification_value, extra_text_value, cot_steps\n",
    "\n",
    "df_cp = df.copy()\n",
    "df_cp[['CoT', 'model_answer', 'alternative_response', 'justification', 'extra_text', 'cot_steps']] = df_cp['output'].apply(lambda x: pd.Series(extract_info(x)))\n",
    "df_cp = df_cp.drop('output', axis=1)\n",
    "df_cp['model_alternative_answer'] = df_cp.apply(lambda row: 'E' if row['alternative_response'] is not None else row['model_answer'], axis=1)\n",
    "df_cp['hit'] = (df_cp['answer'] == df_cp['model_answer']).astype(int)\n",
    "df_cp['hit_alternative'] = (df_cp['answer'] == df_cp['model_alternative_answer']).astype(int)\n",
    "df_cp = df_cp[['source', 'item', 'answer', 'r', 'model', 'prompt_type', 'model_answer', 'hit', 'model_alternative_answer', 'hit_alternative', 'alternative_response', 'justification', 'extra_text', 'CoT', 'cot_steps']]\n",
    "df_cp.to_csv('data/tidydata2cmr.csv', index=False)\n",
    "\n",
    "display(df_cp.head())\n",
    "display(df_cp.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classical Metrics Report - Traditional Approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scikit_posthocs as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from IPython.display import display\n",
    "\n",
    "def calculate_overall_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculates and displays overall metrics.\"\"\"\n",
    "    valid = ~pd.isna(y_true) & ~pd.isna(y_pred)\n",
    "    y_true = y_true[valid]\n",
    "    y_pred = y_pred[valid]\n",
    "\n",
    "    overall_metrics = {\n",
    "        \"Accuracy\": [round(accuracy_score(y_true, y_pred), 4)],\n",
    "        \"Precision\": [round(precision_score(y_true, y_pred, average='weighted', zero_division=0), 4)],\n",
    "        \"Recall\": [round(recall_score(y_true, y_pred, average='weighted', zero_division=0), 4)],\n",
    "        \"F1-Score\": [round(f1_score(y_true, y_pred, average='weighted', zero_division=0), 4)]\n",
    "    }\n",
    "\n",
    "    print(\"ðŸ”¹ Overall Metrics:\")\n",
    "    display(pd.DataFrame(overall_metrics))\n",
    "    return y_true, y_pred\n",
    "\n",
    "def calculate_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"Calculates and displays the confusion matrix.\"\"\"\n",
    "    labels_all = sorted(list(set(y_true) | set(y_pred)))\n",
    "    cm = pd.DataFrame(confusion_matrix(y_true, y_pred, labels=labels_all), index=labels_all, columns=labels_all)\n",
    "    print(\"ðŸ”¹ Overall Confusion Matrix:\")\n",
    "    display(cm)\n",
    "    return labels_all\n",
    "\n",
    "def calculate_group_metrics(df, group_column, answer_col='answer', model_answer_col='model_answer'):\n",
    "    \"\"\"Calculates and displays aggregated metrics by group.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        group_column (str): The name of the column to group by.\n",
    "        answer_col (str): The column containing the true answers.\n",
    "        model_answer_col (str): The column containing the model's answers.\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”¹ Aggregated Metrics by {group_column}:\")\n",
    "\n",
    "    group_metrics = []\n",
    "    group_names = df[group_column].unique()\n",
    "\n",
    "    for group_name in group_names:\n",
    "        group_data = df[df[group_column] == group_name]\n",
    "        yt = group_data[answer_col].to_numpy()\n",
    "        yp = group_data[model_answer_col].to_numpy()\n",
    "        valid = ~pd.isna(yt) & ~pd.isna(yp)\n",
    "        yt = yt[valid]\n",
    "        yp = yp[valid]\n",
    "\n",
    "        if len(yt) == 0:\n",
    "            continue\n",
    "\n",
    "        report = classification_report(yt, yp, output_dict=True, zero_division=0)\n",
    "\n",
    "        row = {\n",
    "            group_column.capitalize(): group_name,\n",
    "            \"Accuracy\": round(report.get(\"accuracy\", np.nan), 4),\n",
    "            \"Macro_Precision\": round(report[\"macro avg\"][\"precision\"], 4),\n",
    "            \"Macro_Recall\": round(report[\"macro avg\"][\"recall\"], 4),\n",
    "            \"Macro_F1\": round(report[\"macro avg\"][\"f1-score\"], 4),\n",
    "            \"Weighted_Precision\": round(report[\"weighted avg\"][\"precision\"], 4),\n",
    "            \"Weighted_Recall\": round(report[\"weighted avg\"][\"recall\"], 4),\n",
    "            \"Weighted_F1\": round(report[\"weighted avg\"][\"f1-score\"], 4)\n",
    "        }\n",
    "\n",
    "        group_metrics.append(row)\n",
    "\n",
    "    df_summary = pd.DataFrame(group_metrics)\n",
    "    display(df_summary)\n",
    "    return group_names\n",
    "\n",
    "def calculate_f1_matrix(df, labels_all, group_names, group_column, answer_col='answer', model_answer_col='model_answer'):\n",
    "    \"\"\"Calculates and displays the F1 matrix per class.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        labels_all (list): List of all possible labels.\n",
    "        group_names (list): List of group names.\n",
    "        group_column (str): The name of the column to group by.\n",
    "        answer_col (str): The column containing the true answers.\n",
    "        model_answer_col (str): The column containing the model's answers.\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”¹ Class-level F1-Score Matrix by {group_column}:\")\n",
    "\n",
    "    f1_matrix = []\n",
    "\n",
    "    for label in labels_all:\n",
    "        f1_row = []\n",
    "        for group_name in group_names:\n",
    "            data = df[df[group_column] == group_name]\n",
    "            yt = data[answer_col].to_numpy()\n",
    "            yp = data[model_answer_col].to_numpy()\n",
    "            valid = ~pd.isna(yt) & ~pd.isna(yp)\n",
    "            yt = yt[valid]\n",
    "            yp = yp[valid]\n",
    "\n",
    "            if len(yt) == 0:\n",
    "                f1_row.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            report = classification_report(yt, yp, output_dict=True, zero_division=0)\n",
    "            f1 = report.get(label, {}).get('f1-score', 0.0)\n",
    "            f1_row.append(round(f1, 4))\n",
    "\n",
    "        f1_matrix.append(f1_row)\n",
    "\n",
    "    f1_scores = np.array(f1_matrix)\n",
    "    df_f1_matrix = pd.DataFrame(f1_scores, index=labels_all, columns=group_names)\n",
    "    display(df_f1_matrix)\n",
    "    return f1_scores, group_names\n",
    "\n",
    "def perform_friedman_nemenyi(f1_scores, group_names):\n",
    "    \"\"\"Performs Friedman test and Nemenyi post-hoc test.\"\"\"\n",
    "    # Filter valid groups\n",
    "    f1_scores_clean = np.array(f1_scores)\n",
    "    valid_columns = ~np.isnan(f1_scores_clean).any(axis=0)\n",
    "    f1_scores_clean = f1_scores_clean[:, valid_columns]\n",
    "    valid_group_names = np.array(group_names)[valid_columns]\n",
    "\n",
    "    # Friedman test\n",
    "    print(\"ðŸ”¹ Friedman Test on Class-level F1-Scores:\")\n",
    "    stat, p = stats.friedmanchisquare(*f1_scores_clean)\n",
    "    print(f\"  Ï‡Â² = {stat:.3f}, p = {p:.4f}\")\n",
    "\n",
    "    # Nemenyi post-hoc test\n",
    "    if p < 0.05:\n",
    "        print(\"ðŸ”¹ Nemenyi Post-hoc Test (p < 0.05):\")\n",
    "        nemenyi_result = sp.posthoc_nemenyi_friedman(f1_scores_clean)\n",
    "        display(nemenyi_result)\n",
    "        print(valid_group_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/tidydata2cmr.csv')\n",
    "df['model_prompt_type'] = df['model'] + '_' + df['prompt_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_col = 'answer'\n",
    "model_answer_col = 'model_answer'\n",
    "\n",
    "\n",
    "# --- Overall metrics ---\n",
    "group_type = 'prompt_type'\n",
    "y_true = df[answer_col].to_numpy()\n",
    "y_pred = df[model_answer_col].to_numpy()\n",
    "\n",
    "valid = ~pd.isna(y_true) & ~pd.isna(y_pred)\n",
    "y_true = y_true[valid]\n",
    "y_pred = y_pred[valid]\n",
    "\n",
    "calculate_overall_metrics(y_true, y_pred)\n",
    "\n",
    "# --- Confusion matrix ---\n",
    "labels_all = calculate_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# --- Aggregated metrics by group ---\n",
    "group_names = calculate_group_metrics(df, group_type, answer_col, model_answer_col)\n",
    "f1_scores, group_names = calculate_f1_matrix(df, labels_all, group_names, group_type, answer_col, model_answer_col)\n",
    "\n",
    "perform_friedman_nemenyi(f1_scores, group_names)\n",
    "\n",
    "\n",
    "# --- Overall metrics ---\n",
    "group_type = 'model'\n",
    "y_true = df[answer_col].to_numpy()\n",
    "y_pred = df[model_answer_col].to_numpy()\n",
    "\n",
    "valid = ~pd.isna(y_true) & ~pd.isna(y_pred)\n",
    "y_true = y_true[valid]\n",
    "y_pred = y_pred[valid]\n",
    "\n",
    "calculate_overall_metrics(y_true, y_pred)\n",
    "\n",
    "# --- Confusion matrix ---\n",
    "labels_all = calculate_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# --- Aggregated metrics by group ---\n",
    "group_names = calculate_group_metrics(df, group_type, answer_col, model_answer_col)\n",
    "f1_scores, group_names = calculate_f1_matrix(df, labels_all, group_names, group_type, answer_col, model_answer_col)\n",
    "\n",
    "perform_friedman_nemenyi(f1_scores, group_names)\n",
    "\n",
    "\n",
    "# --- Overall metrics ---\n",
    "group_type = 'model_prompt_type'\n",
    "y_true = df[answer_col].to_numpy()\n",
    "y_pred = df[model_answer_col].to_numpy()\n",
    "\n",
    "valid = ~pd.isna(y_true) & ~pd.isna(y_pred)\n",
    "y_true = y_true[valid]\n",
    "y_pred = y_pred[valid]\n",
    "\n",
    "calculate_overall_metrics(y_true, y_pred)\n",
    "\n",
    "# --- Confusion matrix ---\n",
    "labels_all = calculate_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# --- Aggregated metrics by group ---\n",
    "group_names = calculate_group_metrics(df, group_type, answer_col, model_answer_col)\n",
    "f1_scores, group_names = calculate_f1_matrix(df, labels_all, group_names, group_type, answer_col, model_answer_col)\n",
    "\n",
    "perform_friedman_nemenyi(f1_scores, group_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_col = 'answer'\n",
    "model_answer_col = 'model_alternative_answer'\n",
    "\n",
    "\n",
    "# --- Overall metrics ---\n",
    "group_type = 'prompt_type'\n",
    "y_true = df[answer_col].to_numpy()\n",
    "y_pred = df[model_answer_col].to_numpy()\n",
    "\n",
    "valid = ~pd.isna(y_true) & ~pd.isna(y_pred)\n",
    "y_true = y_true[valid]\n",
    "y_pred = y_pred[valid]\n",
    "\n",
    "calculate_overall_metrics(y_true, y_pred)\n",
    "\n",
    "# --- Confusion matrix ---\n",
    "labels_all = calculate_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# --- Aggregated metrics by group ---\n",
    "group_names = calculate_group_metrics(df, group_type, answer_col, model_answer_col)\n",
    "f1_scores, group_names = calculate_f1_matrix(df, labels_all, group_names, group_type, answer_col, model_answer_col)\n",
    "\n",
    "perform_friedman_nemenyi(f1_scores, group_names)\n",
    "\n",
    "\n",
    "# --- Overall metrics ---\n",
    "group_type = 'model'\n",
    "y_true = df[answer_col].to_numpy()\n",
    "y_pred = df[model_answer_col].to_numpy()\n",
    "\n",
    "valid = ~pd.isna(y_true) & ~pd.isna(y_pred)\n",
    "y_true = y_true[valid]\n",
    "y_pred = y_pred[valid]\n",
    "\n",
    "calculate_overall_metrics(y_true, y_pred)\n",
    "\n",
    "# --- Confusion matrix ---\n",
    "labels_all = calculate_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# --- Aggregated metrics by group ---\n",
    "group_names = calculate_group_metrics(df, group_type, answer_col, model_answer_col)\n",
    "f1_scores, group_names = calculate_f1_matrix(df, labels_all, group_names, group_type, answer_col, model_answer_col)\n",
    "\n",
    "perform_friedman_nemenyi(f1_scores, group_names)\n",
    "\n",
    "\n",
    "# --- Overall metrics ---\n",
    "group_type = 'model_prompt_type'\n",
    "y_true = df[answer_col].to_numpy()\n",
    "y_pred = df[model_answer_col].to_numpy()\n",
    "\n",
    "valid = ~pd.isna(y_true) & ~pd.isna(y_pred)\n",
    "y_true = y_true[valid]\n",
    "y_pred = y_pred[valid]\n",
    "\n",
    "calculate_overall_metrics(y_true, y_pred)\n",
    "\n",
    "# --- Confusion matrix ---\n",
    "labels_all = calculate_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# --- Aggregated metrics by group ---\n",
    "group_names = calculate_group_metrics(df, group_type, answer_col, model_answer_col)\n",
    "f1_scores, group_names = calculate_f1_matrix(df, labels_all, group_names, group_type, answer_col, model_answer_col)\n",
    "\n",
    "perform_friedman_nemenyi(f1_scores, group_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def calculate_weighted_f1_by_model_and_prompt(df, model_col, prompt_col, answer_col, model_answer_col, n_iterations=100):\n",
    "    \"\"\"\n",
    "    Calculates weighted F1-score for each (model, prompt_type) combination using bootstrap sampling.\n",
    "    Returns a DataFrame with columns: model, prompt, weighted_f1 (one per bootstrap).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for (model, prompt), group in df.groupby([model_col, prompt_col]):\n",
    "        for _ in range(n_iterations):\n",
    "            sample = resample(group, replace=True)\n",
    "            y_true = sample[answer_col].to_numpy()\n",
    "            y_pred = sample[model_answer_col].to_numpy()\n",
    "            valid = ~pd.isna(y_true) & ~pd.isna(y_pred)\n",
    "            y_true = y_true[valid]\n",
    "            y_pred = y_pred[valid]\n",
    "            if len(y_true) > 0 and len(np.unique(y_true)) > 1:\n",
    "                score = f1_score(y_true, y_pred, average='weighted')\n",
    "                results.append({'model': model, 'prompt': prompt, 'weighted_f1': score})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def plot_weighted_f1_by_model_and_prompt(f1_df, title=\"Weighted F1-score by model and prompt type (CMR)\"):\n",
    "    \"\"\"\n",
    "    Plots a grouped barplot with error bars (std deviation) by model and prompt.\n",
    "    \"\"\"\n",
    "    model_name_map = {\n",
    "        \"model_gpt_4o_mini\": \"4o-mini\",\n",
    "        \"model_gpt_41_nano\": \"4.1-nano\",\n",
    "        \"model_claude_35_haiku\": \"3-haiku\",\n",
    "        \"model_grok_3_mini_beta\": \"grok-3-mini\",\n",
    "        \"model_ds_v3\": \"ds-v3\"\n",
    "    }\n",
    "\n",
    "    f1_df = f1_df.dropna()\n",
    "    f1_df['model'] = f1_df['model'].map(model_name_map)\n",
    "\n",
    "    summary_df = (\n",
    "        f1_df.groupby(['model', 'prompt'])\n",
    "        .agg(mean_f1=('weighted_f1', 'mean'), std_f1=('weighted_f1', 'std'))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    model_order = [\"4o-mini\", \"4.1-nano\", \"3-haiku\", \"grok-3-mini\", \"ds-v3\"]\n",
    "    prompt_order = ['adversarial', 'cot', 'naive']\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.barplot(\n",
    "        data=summary_df,\n",
    "        x='model',\n",
    "        y='mean_f1',\n",
    "        hue='prompt',\n",
    "        order=model_order,\n",
    "        hue_order=prompt_order,\n",
    "        palette=['#FFC107', '#FF5722', '#F44336'],\n",
    "        errorbar=None  # Desativa o erro padrÃ£o do seaborn\n",
    "    )\n",
    "\n",
    "    # Adiciona manualmente as barras de erro\n",
    "    for i, row in summary_df.iterrows():\n",
    "        x_pos = model_order.index(row['model']) + (prompt_order.index(row['prompt']) - 1) * 0.25\n",
    "        ax.errorbar(\n",
    "            x=x_pos,\n",
    "            y=row['mean_f1'],\n",
    "            yerr=row['std_f1'],\n",
    "            fmt='none',\n",
    "            c='black',\n",
    "            capsize=5,\n",
    "            linewidth=1\n",
    "        )\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(\"Weighted F1-score\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(title='Prompt')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# ExecuÃ§Ã£o\n",
    "model_col = 'model'\n",
    "prompt_col = 'prompt_type'\n",
    "answer_col = 'answer'\n",
    "model_answer_col = 'model_answer'\n",
    "\n",
    "f1_df = calculate_weighted_f1_by_model_and_prompt(\n",
    "    df, model_col, prompt_col, answer_col, model_answer_col, n_iterations=1000\n",
    ")\n",
    "\n",
    "plot_weighted_f1_by_model_and_prompt(f1_df)\n",
    "plt.savefig('figures/cmr_weighted_f1_scores.png', dpi=360)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def calculate_weighted_f1_by_model_and_prompt(df, model_col, prompt_col, answer_col, model_answer_col, n_iterations=100):\n",
    "    \"\"\"\n",
    "    Calculates weighted F1-score for each (model, prompt_type) combination using bootstrap sampling.\n",
    "    Returns a DataFrame with columns: model, prompt, weighted_f1 (one per bootstrap).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for (model, prompt), group in df.groupby([model_col, prompt_col]):\n",
    "        for _ in range(n_iterations):\n",
    "            sample = resample(group, replace=True)\n",
    "            y_true = sample[answer_col].to_numpy()\n",
    "            y_pred = sample[model_answer_col].to_numpy()\n",
    "            valid = ~pd.isna(y_true) & ~pd.isna(y_pred)\n",
    "            y_true = y_true[valid]\n",
    "            y_pred = y_pred[valid]\n",
    "            if len(y_true) > 0 and len(np.unique(y_true)) > 1:\n",
    "                score = f1_score(y_true, y_pred, average='weighted')\n",
    "                results.append({'model': model, 'prompt': prompt, 'weighted_f1': score})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def plot_weighted_f1_by_model_and_prompt(f1_df, title=\"Weighted F1-score by model and prompt type (a)\"):\n",
    "    \"\"\"\n",
    "    Plots a grouped barplot with error bars (std deviation) by model and prompt.\n",
    "    Also prints bootstrap mean and std estimates as tables.\n",
    "    \"\"\"\n",
    "    model_name_map = {\n",
    "        \"model_gpt_4o_mini\": \"4o-mini\",\n",
    "        \"model_gpt_41_nano\": \"4.1-nano\",\n",
    "        \"model_claude_35_haiku\": \"3-haiku\",\n",
    "        \"model_grok_3_mini_beta\": \"grok-3-mini\",\n",
    "        \"model_ds_v3\": \"ds-v3\"\n",
    "    }\n",
    "\n",
    "    f1_df = f1_df.dropna()\n",
    "    f1_df['model'] = f1_df['model'].map(model_name_map)\n",
    "\n",
    "    summary_df = (\n",
    "        f1_df.groupby(['model', 'prompt'])\n",
    "        .agg(mean_f1=('weighted_f1', 'mean'), std_f1=('weighted_f1', 'std'))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    model_order = [\"4o-mini\", \"4.1-nano\", \"3-haiku\", \"grok-3-mini\", \"ds-v3\"]\n",
    "    prompt_order = ['adversarial', 'cot', 'naive']\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # First plot\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    sns.barplot(\n",
    "        data=summary_df,\n",
    "        x='model',\n",
    "        y='mean_f1',\n",
    "        hue='prompt',\n",
    "        order=model_order,\n",
    "        hue_order=prompt_order,\n",
    "        palette=['#FFC107', '#FF5722', '#F44336'],\n",
    "        errorbar=None  # Disable standard error from seaborn\n",
    "    )\n",
    "\n",
    "    # Add error bars manually\n",
    "    for i, row in summary_df.iterrows():\n",
    "        x_pos = model_order.index(row['model']) + (prompt_order.index(row['prompt']) - 1) * 0.25\n",
    "        ax1.errorbar(\n",
    "            x=x_pos,\n",
    "            y=row['mean_f1'],\n",
    "            yerr=row['std_f1'],\n",
    "            fmt='none',\n",
    "            c='black',\n",
    "            capsize=5,\n",
    "            linewidth=1\n",
    "        )\n",
    "\n",
    "    ax1.set_title(title)\n",
    "    ax1.set_xlabel(\"Model\")\n",
    "    ax1.set_ylabel(\"Weighted F1-score\")\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.legend(title='Prompt')\n",
    "    \n",
    "    # Print bootstrap estimates table for first plot\n",
    "    print(\"Bootstrap mean and std estimates for Model x Prompt (First Plot):\")\n",
    "    for model in model_order:\n",
    "        for prompt in prompt_order:\n",
    "            subset = f1_df[(f1_df['model'] == model) & (f1_df['prompt'] == prompt)]\n",
    "            mean_estimate = subset['weighted_f1'].mean()\n",
    "            std_estimate = subset['weighted_f1'].std()\n",
    "            print(f\"Model: {model}, Prompt: {prompt} -> Mean: {mean_estimate:.4f}, SD: {std_estimate:.4f}\")\n",
    "\n",
    "    # Second plot\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    barplot = sns.barplot(\n",
    "        data=summary_df,\n",
    "        x='prompt',\n",
    "        y='mean_f1',\n",
    "        hue='model',\n",
    "        order=prompt_order,\n",
    "        palette='Blues',\n",
    "        ax=ax2,\n",
    "        errorbar=None\n",
    "    )\n",
    "\n",
    "    # Add error bars in correct position\n",
    "    for bar, (_, row) in zip(barplot.patches, summary_df.iterrows()):\n",
    "        x = bar.get_x() + bar.get_width() / 2\n",
    "        y = bar.get_height()\n",
    "        yerr = row['std_f1']\n",
    "        ax2.errorbar(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            yerr=yerr,\n",
    "            fmt='none',\n",
    "            c='black',\n",
    "            capsize=5,\n",
    "            linewidth=1\n",
    "        )\n",
    "\n",
    "    ax2.set_title(\"Weighted F1-score by Prompt (b)\")\n",
    "    ax2.set_xlabel(\"Prompt\")\n",
    "    ax2.set_ylabel(\"Weighted F1-score\")\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.legend(title='Model')\n",
    "\n",
    "    # Print bootstrap estimates table for second plot\n",
    "    print(\"Bootstrap mean and std estimates for Prompt x Model (Second Plot):\")\n",
    "    for prompt in prompt_order:\n",
    "        for model in model_order:\n",
    "            subset = f1_df[(f1_df['model'] == model) & (f1_df['prompt'] == prompt)]\n",
    "            mean_estimate = subset['weighted_f1'].mean()\n",
    "            std_estimate = subset['weighted_f1'].std()\n",
    "            print(f\"Prompt: {prompt}, Model: {model} -> Mean: {mean_estimate:.4f}, SD: {std_estimate:.4f}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "# ExecuÃ§Ã£o\n",
    "model_col = 'model'\n",
    "prompt_col = 'prompt_type'\n",
    "answer_col = 'answer'\n",
    "model_answer_col = 'model_answer'\n",
    "\n",
    "f1_df = calculate_weighted_f1_by_model_and_prompt(\n",
    "    df, model_col, prompt_col, answer_col, model_answer_col, n_iterations=1000\n",
    ")\n",
    "\n",
    "plot_weighted_f1_by_model_and_prompt(f1_df)\n",
    "plt.savefig('figures/cmr_weighted_f1_scores.png', dpi=360)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
