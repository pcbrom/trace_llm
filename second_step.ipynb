{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Second Step: Zero-Shot Semantic Interval Rubric with model-internal evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "# Load .env\n",
    "dotenv_path = \"/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/.env\"\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "# Retrieve API keys\n",
    "openai_api_key     = os.getenv(\"OPENAI_API_KEY\")\n",
    "claude_api_key     = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "xai_api_key        = os.getenv(\"XAI_API_KEY\")\n",
    "deepseek_api_key   = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "# Configure clients\n",
    "client_gpt     = OpenAI(api_key=openai_api_key)\n",
    "client_claude  = anthropic.Anthropic(api_key=claude_api_key)\n",
    "client_grok    = OpenAI(api_key=xai_api_key, base_url=\"https://api.x.ai/v1\")\n",
    "client_ds      = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "# Registry of supported models\n",
    "models = {\n",
    "    'model_gpt_4o_mini': {'alias': '4o-mini', 'model': 'gpt-4o-mini', 'client': client_gpt},\n",
    "    'model_gpt_41_nano': {'alias': '4.1-nano', 'model': 'gpt-4.1-nano', 'client': client_gpt},\n",
    "    'model_claude_35_haiku': {'alias': '3-haiku', 'model': 'claude-3-5-haiku-latest', 'client': client_claude    },\n",
    "    'model_grok_3_mini_beta': {'alias': 'grok-3-mini', 'model': 'grok-3-mini-beta', 'client': client_grok},\n",
    "    'model_ds_v3': {'alias': 'ds-v3', 'model': 'deepseek-chat', 'client': client_ds}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_two(model_name, prompt):\n",
    "    model_config = models.get(model_name)\n",
    "    if not model_config:\n",
    "        raise ValueError(f\"Model '{model_name}' not found in the 'models' dictionary.\")\n",
    "\n",
    "    client = model_config['client']\n",
    "    model = model_config['model']\n",
    "\n",
    "    if isinstance(client, OpenAI):\n",
    "        response = client.chat.completions.create(model=model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        generated_text = response.choices[0].message.content\n",
    "    \n",
    "    elif isinstance(client, anthropic.Anthropic):\n",
    "        response = client.messages.create(model=model, max_tokens=1000, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        generated_text = response.content[0].text\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported client type.\")\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "def step_two_recovered(model_name, prompt):\n",
    "    # Extract model identifier from the model name\n",
    "    model = model_name.split('_', 1)[-1]\n",
    "    model_config = model_name\n",
    "    if not model_config:\n",
    "        raise ValueError(f\"Model '{model_name}' not found in the 'models' dictionary.\")\n",
    "\n",
    "    # Assign the appropriate client and model based on the model name\n",
    "    if model_name == 'model_gpt_4o_mini':\n",
    "        openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        client = OpenAI(api_key=openai_api_key)\n",
    "        model = 'gpt-4o-mini'\n",
    "        \n",
    "    elif model_name == 'model_gpt_41_nano':\n",
    "        openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        client = OpenAI(api_key=openai_api_key)\n",
    "        model = 'gpt-4.1-nano'\n",
    "        \n",
    "    elif model_name == 'model_claude_35_haiku':\n",
    "        claude_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        client = anthropic.Anthropic(api_key=claude_api_key)\n",
    "        model = 'claude-3-5-haiku-latest'\n",
    "        \n",
    "    elif model_name == 'model_grok_3_mini_beta':\n",
    "        xai_api_key = os.getenv(\"XAI_API_KEY\")\n",
    "        client = OpenAI(api_key=xai_api_key, base_url=\"https://api.x.ai/v1\")\n",
    "        model = 'grok-3-mini-beta'\n",
    "\n",
    "    elif model_name == 'model_ds_v3':\n",
    "        deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "        client = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")\n",
    "        model = 'deepseek-chat'\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model name: {model_name}\")\n",
    "\n",
    "    # Generate response based on client type\n",
    "    if isinstance(client, OpenAI):\n",
    "        response = client.chat.completions.create(model=model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        generated_text = response.choices[0].message.content\n",
    "    elif isinstance(client, anthropic.Anthropic):\n",
    "        response = client.messages.create(model=model, max_tokens=1000, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        generated_text = response.content[0].text\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported client type.\")\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV data into a DataFrame\n",
    "df = pd.read_csv('data/tidydata2cmr.csv').fillna('')\n",
    "print(df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Depth of Reasoning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_depth_of_reasoning_prompt(row):\n",
    "    prompt = (\n",
    "        \"You are a professional evaluator. Your task is to assess the Depth of Reasoning of the extracted content.\"\n",
    "        \"Assess the complexity and structure of the reasoning.\\n\\n\"\n",
    "        \"Input:\\n\\n\"\n",
    "        \"- Item: \" f\"{row[\"item\"]}\\n\"\n",
    "        \"- Correct Answer: \" f\"{row[\"answer\"]}\\n\\n\"\n",
    "        \"- Extracted Answer: \" f\"{row[\"model_alternative_answer\"]}\\n\"\n",
    "        \"- Extracted Justification: \" f\"{row[\"justification\"]}\\n\"\n",
    "        \"- Extracted Alternative Response: \" f\"{row[\"alternative_response\"]}\\n\"\n",
    "        \"- Extracted Chain of Thought: \" f\"{row[\"CoT\"]}\\n\\n\"\n",
    "\n",
    "        \"Score from 0 (no reasoning) to 10 (multi-step, layered inference).\\n\\n\"\n",
    "\n",
    "        \"Refer to the following rubric:\\n\\n\"\n",
    "        \"Score Interval: (9,10] | Multi-step, layered reasoning with strong inferential structure.\\n\"\n",
    "        \"Score Interval: (7,9]  | Sound and structured reasoning with some complexity.\\n\"\n",
    "        \"Score Interval: (5,7]  | Basic logical sequence with minimal elaboration.\\n\"\n",
    "        \"Score Interval: (3,5]  | Shallow reasoning with gaps or simplifications.\\n\"\n",
    "        \"Score Interval: (1,3]  | Fragmented logic or one-step heuristic answer.\\n\"\n",
    "        \"Score Interval: [0,1]  | No reasoning trace or incoherent rationale.\\n\\n\"\n",
    "\n",
    "        \"Based on this rubric, assign a score from 0 to 10 for Depth of Reasoning\\n\"\n",
    "        \"Return ONLY the following JSON:\\n\\n\"\n",
    "        \n",
    "        \"```json\\n\"\n",
    "        \"{\\n\"\n",
    "            \"depth_of_reasoning: <float>,\\n\"\n",
    "            \"depth_of_reasoning_justification: <explanation of reasoning structure>\\n\"\n",
    "        \"}\\n\"\n",
    "        \"```\\n\\n\"\n",
    "        \"Ensure the JSON is parseable by a standard JSON parser (double quotes for keys, no trailing commas).\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "i=0\n",
    "row = df.iloc[i]\n",
    "print(build_depth_of_reasoning_prompt(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique model names from the dataframe\n",
    "unique_models = df['model'].unique()\n",
    "\n",
    "# Prepare subsets of the dataframe for each target model\n",
    "subsets_with_results = {\n",
    "    model: df[df['model'] == model].copy()\n",
    "    for model in unique_models\n",
    "}\n",
    "\n",
    "# Function to evaluate a pair of models (evaluator and target)\n",
    "def evaluate_pair(evaluator_model, target_model):\n",
    "    subset = subsets_with_results[target_model]\n",
    "    generated = []\n",
    "\n",
    "    # Iterate over each row in the subset with progress bar\n",
    "    for _, row in tqdm(subset.iterrows(),\n",
    "                       total=len(subset),\n",
    "                       desc=f\"{evaluator_model} â†’ {target_model}\",\n",
    "                       leave=False):\n",
    "        # Build prompt for depth of reasoning assessment\n",
    "        prompt = build_depth_of_reasoning_prompt(row)\n",
    "        # Generate response using step_two function\n",
    "        generated_text = step_two(evaluator_model, prompt)\n",
    "        generated.append(generated_text)\n",
    "\n",
    "    # Store generated responses in the corresponding column\n",
    "    subsets_with_results[target_model][f'step2_{evaluator_model}'] = generated\n",
    "\n",
    "# Create list of tasks for all model pairs (evaluator vs target)\n",
    "tasks = [\n",
    "    (evaluator, target)\n",
    "    for evaluator in unique_models\n",
    "    for target in unique_models\n",
    "]\n",
    "\n",
    "# Execute evaluation tasks in parallel using ThreadPoolExecutor\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(evaluate_pair, evaluator, target)\n",
    "        for evaluator, target in tasks\n",
    "    ]\n",
    "    # Wait for all tasks to complete\n",
    "    concurrent.futures.wait(futures)\n",
    "\n",
    "# Save the results for each target model to CSV files\n",
    "for target_model, df_result in subsets_with_results.items():\n",
    "    df_result.to_csv(f\"data/step2/depth_of_reasoning_{target_model}.csv\", index=False)\n",
    "    print(f\"Saved: depth_of_reasoning_{target_model}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique models from the dataframe\n",
    "unique_models = df['model'].unique()\n",
    "\n",
    "# Create subsets of the dataframe for each target model\n",
    "subsets_with_results = {\n",
    "    model: df[df['model'] == model].copy()\n",
    "    for model in unique_models\n",
    "}\n",
    "\n",
    "# List of missing model pairs (evaluator â†’ target)\n",
    "missing_pairs = [\n",
    "    (\"model_gpt_4o_mini\", \"model_gpt_41_nano\"),\n",
    "    (\"model_gpt_4o_mini\", \"model_ds_v3\"),\n",
    "    (\"model_gpt_4o_mini\", \"model_gpt_4o_mini\"),\n",
    "    (\"model_gpt_41_nano\", \"model_claude_35_haiku\"),\n",
    "    (\"model_claude_35_haiku\", \"model_gpt_4o_mini\"),\n",
    "    (\"model_gpt_41_nano\", \"model_gpt_4o_mini\"),\n",
    "    (\"model_gpt_4o_mini\", \"model_claude_35_haiku\"),\n",
    "]\n",
    "\n",
    "# Output directory path\n",
    "output_dir = \"data/step2\"\n",
    "\n",
    "# Main evaluation function for each model pair\n",
    "def evaluate_pair(evaluator_model, target_model):\n",
    "    subset = subsets_with_results[target_model]\n",
    "    generated = []\n",
    "\n",
    "    print(f\"\\nEvaluating: {evaluator_model} â†’ {target_model} ({len(subset)} items)\")    \n",
    "    for _, row in tqdm(subset.iterrows(), total=len(subset), desc=f\"{evaluator_model} â†’ {target_model}\", leave=False):\n",
    "        try:\n",
    "            # Build prompt for depth of reasoning assessment\n",
    "            prompt = build_depth_of_reasoning_prompt(row)\n",
    "            # Generate response using the recovery function\n",
    "            generated_text = step_two_recovered(evaluator_model, prompt)\n",
    "        except Exception as e:\n",
    "            # Handle errors gracefully\n",
    "            generated_text = f\"ERROR: {str(e)}\"\n",
    "        generated.append(generated_text)\n",
    "\n",
    "    # Store generated responses in the corresponding column\n",
    "    subsets_with_results[target_model][f'step2_{evaluator_model}'] = generated\n",
    "\n",
    "# Execute evaluation for each missing pair in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(evaluate_pair, evaluator, target)\n",
    "        for evaluator, target in missing_pairs\n",
    "    ]\n",
    "    # Wait for all tasks to complete\n",
    "    concurrent.futures.wait(futures)\n",
    "\n",
    "# Save each target model's results as a separate CSV to prevent overwriting\n",
    "for target_model, df_result in subsets_with_results.items():\n",
    "    output_path = os.path.join(output_dir, f\"depth_of_reasoning_{target_model}_recovered.csv\")\n",
    "    df_result.to_csv(output_path, index=False)\n",
    "    print(f\"Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the list of CSV files alphabetically and print\n",
    "step2_dir = \"data/step2\"\n",
    "csv_files = [f for f in os.listdir(step2_dir) if f.endswith('.csv')]\n",
    "csv_files.sort()\n",
    "print(csv_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Caminho para os arquivos CSV\n",
    "step2_dir = 'data/step2/partial'\n",
    "output_dir = 'data/step2/merged'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Lista de arquivos CSV na pasta (excluindo os que jÃ¡ sÃ£o \"_recovered.csv\")\n",
    "csv_files = [f for f in os.listdir(step2_dir) if f.endswith('.csv') and not f.endswith('_recovered.csv')]\n",
    "\n",
    "# Colunas para realizar o join\n",
    "join_columns = [\n",
    "    'source', 'item', 'answer', 'r', 'model', 'prompt_type',\n",
    "    'model_answer', 'hit', 'model_alternative_answer',\n",
    "    'hit_alternative', 'alternative_response', 'justification',\n",
    "    'extra_text', 'CoT', 'cot_steps'\n",
    "]\n",
    "\n",
    "# Lista para acumular todos os merged_df\n",
    "merged_list = []\n",
    "\n",
    "for filename in csv_files:\n",
    "    original_path = os.path.join(step2_dir, filename)\n",
    "    recovered_path = os.path.join(step2_dir, filename.replace('.csv', '_recovered.csv'))\n",
    "    \n",
    "    if not os.path.exists(recovered_path):\n",
    "        print(f'Arquivo nÃ£o encontrado: {recovered_path}')\n",
    "        continue\n",
    "\n",
    "    original_df = pd.read_csv(original_path)\n",
    "    recovered_df = pd.read_csv(recovered_path)\n",
    "\n",
    "    merged_df = original_df.merge(recovered_df, how='left', on=join_columns)\n",
    "\n",
    "    # Reorganize columns apÃ³s 'cot_steps'\n",
    "    cols = list(merged_df.columns)\n",
    "    try:\n",
    "        cot_steps_index = cols.index('cot_steps')\n",
    "        after_cot_steps = cols[cot_steps_index + 1:]\n",
    "        sorted_after_cot_steps = sorted(after_cot_steps)\n",
    "        new_order = cols[:cot_steps_index + 1] + sorted_after_cot_steps\n",
    "        merged_df = merged_df[new_order]\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    # Salva o arquivo individual\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    print(merged_df.shape)\n",
    "\n",
    "    # Acumula no merged_all\n",
    "    merged_list.append(merged_df)\n",
    "\n",
    "# Concatena tudo no final\n",
    "merged_all = pd.concat(merged_list, ignore_index=True)\n",
    "merged_all.to_csv(os.path.join(output_dir, 'depth_of_reasoning_model_merged_all.csv'), index=False)\n",
    "print('merged_all.csv salvo com shape:', merged_all.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Originality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_originality_prompt(row):\n",
    "    prompt = (\n",
    "        \"You are a professional evaluator. Your task is to assess the Originality of the extracted content. \"\n",
    "        \"Evaluate the degree of abstraction, rephrasing, and lexical and syntactic diversity. Identify signs of prompt \"\n",
    "        \"echoing, i.e., shallow repetition of lexical elements or structural patterns from the input.\\n\\n\"\n",
    "\n",
    "        \"Input:\\n\"\n",
    "        f\"- Item: {row['item']}\\n\"\n",
    "        f\"- Extracted Answer: {row['model_alternative_answer']}\\n\"\n",
    "        f\"- Extracted Justification: {row['justification']}\\n\"\n",
    "        f\"- Extracted Alternative Response: {row['alternative_response']}\\n\"\n",
    "        f\"- Extracted Chain of Thought: {row['CoT']}\\n\\n\"\n",
    "\n",
    "        \"Also consider the following patterns indicative of originality:\\n\"\n",
    "        \"- Use of intermediate planning (e.g., step-by-step comments or structured reasoning).\\n\"\n",
    "        \"- Iterative reasoning or trial-and-error, especially evident in CoT or alternative responses.\\n\"\n",
    "        \"- Reformulation that departs from both lexical and syntactic structure of the input.\\n\"\n",
    "        \"- Introduction of new abstractions, analogies, or contextual elements not present in the original item.\\n\"\n",
    "        \"Flag low originality if responses mirror the input structure or vocabulary, even with surface paraphrasing.\\n\\n\"\n",
    "\n",
    "        \"Score from 0 (verbatim or generic) to 10 (highly novel or creative).\\n\\n\"\n",
    "        \"Refer to the following refined rubric:\\n\"\n",
    "\n",
    "        \"Score Interval: (9,10]  | Highly original and creative phrasing. Demonstrates abstraction, analogies, or planning. \"\n",
    "        \"No lexical or structural echo from the prompt.\\n\"\n",
    "        \"Score Interval: (7,9]   | Strong semantic transformation with novel structure or reframing. Limited reuse of form or \"\n",
    "        \"vocabulary is acceptable if embedded in creative formulation.\\n\"\n",
    "        \"Score Interval: (5,7]   | Contains some original phrasing or ideas, but retains significant elements from the prompt's \"\n",
    "        \"syntax or lexicon. Limited abstraction.\\n\"\n",
    "        \"Score Interval: (3,5]   | Mostly generic or templated response. Moderate echoing in either vocabulary or sentence structure.\\n\"\n",
    "        \"Score Interval: (1,3]   | Prompt echoing dominates, including structural mimicry. Very low novelty.\\n\"\n",
    "        \"Score Interval: [0,1]   | Verbatim or near-verbatim reproduction. No abstraction or transformation detected.\\n\\n\"\n",
    "\n",
    "        \"Based on this rubric, assign a score from 0 to 10 for Originality.\\n\"\n",
    "        \"Return ONLY the following JSON:\\n\\n\"\n",
    "        \"```json\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"originality\": <float>,\\n'\n",
    "        '  \"originality_justification\": \"<brief statement of novelty or echoing>\"\\n'\n",
    "        \"}\\n\"\n",
    "        \"```\\n\\n\"\n",
    "        \"Ensure the JSON is parseable by a standard JSON parser (double quotes for keys, no trailing commas).\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "i=0\n",
    "row = df.iloc[i]\n",
    "print(build_originality_prompt(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique model names from the dataframe \n",
    "unique_models = df['model'].unique()\n",
    "\n",
    "# Prepare subsets of the dataframe for each target model\n",
    "subsets_with_results = {\n",
    "    model: df[df['model'] == model].copy()\n",
    "    for model in unique_models\n",
    "}\n",
    "\n",
    "# Function to evaluate a pair of models (evaluator and target)\n",
    "def evaluate_pair(evaluator_model, target_model):\n",
    "    subset = subsets_with_results[target_model]\n",
    "    generated = []\n",
    "\n",
    "    # Iterate over each row in the subset with progress bar\n",
    "    for _, row in tqdm(subset.iterrows(),\n",
    "                       total=len(subset),\n",
    "                       desc=f\"{evaluator_model} â†’ {target_model}\",\n",
    "                       leave=False):\n",
    "        # Build prompt for depth of reasoning assessment\n",
    "        prompt = build_originality_prompt(row)\n",
    "        # Generate response using step_two function\n",
    "        generated_text = step_two(evaluator_model, prompt)\n",
    "        generated.append(generated_text)\n",
    "\n",
    "    # Store generated responses in the corresponding column\n",
    "    subsets_with_results[target_model][f'step2_{evaluator_model}'] = generated\n",
    "\n",
    "# Create list of tasks for all model pairs (evaluator vs target)\n",
    "tasks = [\n",
    "    (evaluator, target)\n",
    "    for evaluator in unique_models\n",
    "    for target in unique_models\n",
    "]\n",
    "\n",
    "# Execute evaluation tasks in parallel using ThreadPoolExecutor\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(evaluate_pair, evaluator, target)\n",
    "        for evaluator, target in tasks\n",
    "    ]\n",
    "    # Wait for all tasks to complete\n",
    "    concurrent.futures.wait(futures)\n",
    "\n",
    "# Save the results for each target model to CSV files\n",
    "for target_model, df_result in subsets_with_results.items():\n",
    "    df_result.to_csv(f\"data/step2/partial/originality_{target_model}.csv\", index=False)\n",
    "    print(f\"Saved: originality_{target_model}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique models from the dataframe\n",
    "unique_models = df['model'].unique()\n",
    "\n",
    "# Create subsets of the dataframe for each target model\n",
    "subsets_with_results = {\n",
    "    model: df[df['model'] == model].copy()\n",
    "    for model in unique_models\n",
    "}\n",
    "\n",
    "# List of missing model pairs (evaluator â†’ target)\n",
    "missing_pairs = [\n",
    "    (\"model_gpt_4o_mini\", \"model_gpt_4o_mini\"),\n",
    "    (\"model_gpt_4o_mini\", \"model_gpt_41_nano\"),\n",
    "    (\"model_claude_35_haiku\", \"model_gpt_4o_mini\"),\n",
    "    (\"model_gpt_4o_mini\", \"model_ds_v3\"),\n",
    "    (\"model_gpt_41_nano\", \"model_gpt_41_nano\"),\n",
    "    (\"model_gpt_41_nano\", \"model_gpt_4o_mini\"),\n",
    "    (\"model_gpt_41_nano\", \"model_grok_3_mini_beta\"),\n",
    "    (\"model_gpt_4o_mini\", \"model_claude_35_haiku\"),\n",
    "    (\"model_claude_35_haiku\", \"model_gpt_41_nano\"),\n",
    "    (\"model_claude_35_haiku\", \"model_grok_3_mini_beta\")\n",
    "]\n",
    "\n",
    "# Output directory path\n",
    "output_dir = \"data/step2/partial\"\n",
    "\n",
    "# Main evaluation function for each model pair\n",
    "def evaluate_pair(evaluator_model, target_model):\n",
    "    subset = subsets_with_results[target_model]\n",
    "    generated = []\n",
    "\n",
    "    print(f\"\\nEvaluating: {evaluator_model} â†’ {target_model} ({len(subset)} items)\")    \n",
    "    for _, row in tqdm(subset.iterrows(), total=len(subset), desc=f\"{evaluator_model} â†’ {target_model}\", leave=False):\n",
    "        try:\n",
    "            # Build prompt for depth of reasoning assessment\n",
    "            prompt = build_originality_prompt(row)\n",
    "            # Generate response using the recovery function\n",
    "            generated_text = step_two_recovered(evaluator_model, prompt)\n",
    "        except Exception as e:\n",
    "            # Handle errors gracefully\n",
    "            generated_text = f\"ERROR: {str(e)}\"\n",
    "        generated.append(generated_text)\n",
    "\n",
    "    # Store generated responses in the corresponding column\n",
    "    subsets_with_results[target_model][f'step2_{evaluator_model}'] = generated\n",
    "\n",
    "# Execute evaluation for each missing pair in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(evaluate_pair, evaluator, target)\n",
    "        for evaluator, target in missing_pairs\n",
    "    ]\n",
    "    # Wait for all tasks to complete\n",
    "    concurrent.futures.wait(futures)\n",
    "\n",
    "# Save each target model's results as a separate CSV to prevent overwriting\n",
    "for target_model, df_result in subsets_with_results.items():\n",
    "    output_path = os.path.join(output_dir, f\"originality_{target_model}_recovered.csv\")\n",
    "    df_result.to_csv(output_path, index=False)\n",
    "    print(f\"Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the list of CSV files alphabetically and print\n",
    "step2_dir = \"data/step2/partial\"\n",
    "csv_files = [f for f in os.listdir(step2_dir) if f.endswith('.csv')]\n",
    "csv_files.sort()\n",
    "print(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Caminho para os arquivos CSV\n",
    "step2_dir = 'data/step2/partial'\n",
    "output_dir = 'data/step2/merged'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Lista de arquivos CSV na pasta (excluindo os que jÃ¡ sÃ£o \"_recovered.csv\")\n",
    "csv_files = [f for f in os.listdir(step2_dir) if f.endswith('.csv') and not f.endswith('_recovered.csv')]\n",
    "\n",
    "# Colunas para realizar o join\n",
    "join_columns = [\n",
    "    'source', 'item', 'answer', 'r', 'model', 'prompt_type',\n",
    "    'model_answer', 'hit', 'model_alternative_answer',\n",
    "    'hit_alternative', 'alternative_response', 'justification',\n",
    "    'extra_text', 'CoT', 'cot_steps'\n",
    "]\n",
    "\n",
    "# Lista para acumular todos os merged_df\n",
    "merged_list = []\n",
    "\n",
    "for filename in csv_files:\n",
    "    original_path = os.path.join(step2_dir, filename)\n",
    "    recovered_path = os.path.join(step2_dir, filename.replace('.csv', '_recovered.csv'))\n",
    "    \n",
    "    if not os.path.exists(recovered_path):\n",
    "        print(f'Arquivo nÃ£o encontrado: {recovered_path}')\n",
    "        continue\n",
    "\n",
    "    original_df = pd.read_csv(original_path)\n",
    "    recovered_df = pd.read_csv(recovered_path)\n",
    "\n",
    "    merged_df = original_df.merge(recovered_df, how='left', on=join_columns)\n",
    "\n",
    "    # Reorganize columns apÃ³s 'cot_steps'\n",
    "    cols = list(merged_df.columns)\n",
    "    try:\n",
    "        cot_steps_index = cols.index('cot_steps')\n",
    "        after_cot_steps = cols[cot_steps_index + 1:]\n",
    "        sorted_after_cot_steps = sorted(after_cot_steps)\n",
    "        new_order = cols[:cot_steps_index + 1] + sorted_after_cot_steps\n",
    "        merged_df = merged_df[new_order]\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    # Salva o arquivo individual\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    print(merged_df.shape)\n",
    "\n",
    "    # Acumula no merged_all\n",
    "    merged_list.append(merged_df)\n",
    "\n",
    "# Concatena tudo no final\n",
    "merged_all = pd.concat(merged_list, ignore_index=True)\n",
    "merged_all.to_csv(os.path.join(output_dir, 'originality_model_merged_all.csv'), index=False)\n",
    "print('merged_all.csv salvo com shape:', merged_all.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Recovery ERROR code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the depth of reasoning and originality datasets\n",
    "depth_df = pd.read_csv('data/depth_of_reasoning_model_merged_all.csv').fillna(\"\")\n",
    "originality_df = pd.read_csv('data/originality_model_merged_all.csv').fillna(\"\")\n",
    "\n",
    "# Optional: Display the first few rows to verify successful loading\n",
    "#display(depth_df.head(2))\n",
    "#display(originality_df.head(2))\n",
    "\n",
    "# List all column names in the current DataFrame\n",
    "print(depth_df.columns.tolist())\n",
    "\n",
    "# Create two datasets: one with errors in specified columns, another without errors\n",
    "\n",
    "# Define the columns to check for errors\n",
    "columns_to_check = [\n",
    "    'step2_model_claude_35_haiku',\n",
    "    'step2_model_ds_v3',\n",
    "    'step2_model_gpt_41_nano',\n",
    "    'step2_model_gpt_4o_mini',\n",
    "    'step2_model_grok_3_mini_beta'\n",
    "]\n",
    "\n",
    "import re\n",
    "\n",
    "# Function to identify errors in the specified columns using regex\n",
    "def has_error(value):\n",
    "    if pd.isnull(value):\n",
    "        return False\n",
    "    value_str = str(value)\n",
    "    # Check if the string starts with 'Error' (case-insensitive)\n",
    "    if re.match(r'^ERROR', value_str, re.IGNORECASE):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Filter dataset with errors\n",
    "deep_df_with_errors = depth_df[depth_df[columns_to_check].apply(lambda row: any(has_error(val) for val in row), axis=1)]\n",
    "originality_df_with_errors = originality_df[originality_df[columns_to_check].apply(lambda row: any(has_error(val) for val in row), axis=1)]\n",
    "\n",
    "# Filter dataset without errors\n",
    "deep_df_without_errors = depth_df[~depth_df[columns_to_check].apply(lambda row: any(has_error(val) for val in row), axis=1)]\n",
    "deep_dforiginality_df_without_errors = originality_df[~originality_df[columns_to_check].apply(lambda row: any(has_error(val) for val in row), axis=1)]\n",
    "\n",
    "#display(deep_df_with_errors.head(2))\n",
    "#display(originality_df_with_errors.head(2))\n",
    "deep_df_with_errors_original = deep_df_with_errors.copy(deep=True)\n",
    "originality_df_with_errors_original = originality_df_with_errors.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def recall_llm_and_update_depth(df, model_column):\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=f\"Recalling {model_column}\"):\n",
    "        if str(row[model_column]).startswith(\"ERROR\"):\n",
    "            try:\n",
    "                prompt = build_depth_of_reasoning_prompt(row)\n",
    "                model_to_use = model_column.replace('step2_', '')  # ajusta o nome do modelo\n",
    "                response = step_two_recovered(model_to_use, prompt)\n",
    "                df.at[index, model_column] = response\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {index}: {e}\")\n",
    "                df.at[index, model_column] = f\"ERROR: {e}\"\n",
    "    return df\n",
    "\n",
    "def recall_llm_and_update_originality(df, model_column):\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=f\"Recalling {model_column}\"):\n",
    "        if str(row[model_column]).startswith(\"ERROR\"):\n",
    "            try:\n",
    "                prompt = build_originality_prompt(row)\n",
    "                model_to_use = model_column.replace('step2_', '')  # ajusta o nome do modelo\n",
    "                response = step_two_recovered(model_to_use, prompt)\n",
    "                df.at[index, model_column] = response\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {index}: {e}\")\n",
    "                df.at[index, model_column] = f\"ERROR: {e}\"\n",
    "    return df\n",
    "\n",
    "# Iterate through the columns and apply the recall function\n",
    "for column in columns_to_check:\n",
    "    deep_df_with_errors = recall_llm_and_update_depth(deep_df_with_errors, column)\n",
    "    originality_df_with_errors = recall_llm_and_update_originality(originality_df_with_errors, column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of occurrences where any error exists in the specified columns of depth_df\n",
    "error_deep = deep_df_with_errors[deep_df_with_errors[columns_to_check].apply(lambda row: any(has_error(val) for val in row), axis=1)].shape[0]\n",
    "error_orig = originality_df_with_errors[originality_df_with_errors[columns_to_check].apply(lambda row: any(has_error(val) for val in row), axis=1)].shape[0]\n",
    "print(f\"Number of error occurrences: {error_deep}, {error_orig}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_deep = pd.concat([deep_df_without_errors, deep_df_with_errors], ignore_index=True)\n",
    "df_final_orig = pd.concat([deep_dforiginality_df_without_errors, originality_df_with_errors], ignore_index=True)\n",
    "\n",
    "# Save the concatenated dataframes as CSV files\n",
    "df_final_deep.to_csv('data/deep_recovered.csv', index=False)\n",
    "df_final_orig.to_csv('data/originality_recovered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Extract from cell json - complete and tidy data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the recovered deep data\n",
    "df_dor = pd.read_csv('data/deep_recovered.csv')\n",
    "df_dor = df_dor.fillna(\"\")  # Fill NaN values with empty string\n",
    "\n",
    "# Load the recovered originality data\n",
    "df_ori = pd.read_csv('data/originality_recovered.csv')\n",
    "df_ori = df_ori.fillna(\"\")  # Fill NaN values with empty string\n",
    "\n",
    "# Lista de colunas a processar\n",
    "columns_to_check = [\n",
    "    'step2_model_claude_35_haiku',\n",
    "    'step2_model_ds_v3',\n",
    "    'step2_model_gpt_41_nano',\n",
    "    'step2_model_gpt_4o_mini',\n",
    "    'step2_model_grok_3_mini_beta'\n",
    "]\n",
    "\n",
    "# FunÃ§Ãµes auxiliares com regex\n",
    "def extract_field_regex(text, field):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    pattern_number = rf'\"{field}\"\\s*:\\s*([0-9]+(?:\\.[0-9]+)?)'\n",
    "    pattern_string = rf'\"{field}\"\\s*:\\s*\"([^\"]+)\"'\n",
    "    \n",
    "    match_number = re.search(pattern_number, text)\n",
    "    if match_number:\n",
    "        return float(match_number.group(1))\n",
    "    \n",
    "    match_string = re.search(pattern_string, text)\n",
    "    if match_string:\n",
    "        return match_string.group(1)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Aplica a extraÃ§Ã£o para cada coluna\n",
    "for col in columns_to_check:\n",
    "    model = col.replace('step2_model_', '')\n",
    "\n",
    "    # df_dor: extrai depth_of_reasoning e justification\n",
    "    df_dor[f'gr_dor_{model}'] = df_dor[col].apply(lambda x: extract_field_regex(x, 'depth_of_reasoning'))\n",
    "    df_dor[f'just_dor_{model}'] = df_dor[col].apply(lambda x: extract_field_regex(x, 'depth_of_reasoning_justification'))\n",
    "\n",
    "    # df_ori: extrai originality e justification\n",
    "    df_ori[f'gr_ori_{model}'] = df_ori[col].apply(lambda x: extract_field_regex(x, 'originality'))\n",
    "    df_ori[f'just_ori_{model}'] = df_ori[col].apply(lambda x: extract_field_regex(x, 'originality_justification'))\n",
    "\n",
    "# Adiciona a coluna que identifica a origem da base\n",
    "df_dor['criterion'] = 'depth of reasoning'\n",
    "df_ori['criterion'] = 'originality'\n",
    "\n",
    "df_combined = df_dor.merge(df_ori, on=['source', 'item', 'answer', 'r', 'model', 'prompt_type', 'model_answer',\n",
    "                                        'hit', 'model_alternative_answer', 'hit_alternative',\n",
    "                                        'alternative_response', 'justification', 'extra_text', 'CoT',\n",
    "                                        'cot_steps'], how='left')\n",
    "\n",
    "\n",
    "# Substituir strings vazias ou espaÃ§os por NaN\n",
    "df_combined['cot_steps'] = df_combined['cot_steps'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "# Converter para numÃ©rico (valores invÃ¡lidos serÃ£o convertidos em NaN)\n",
    "df_combined['cot_steps'] = pd.to_numeric(df_combined['cot_steps'], errors='coerce')\n",
    "\n",
    "df_combined.to_parquet('data/tidy_data_2_aed_model.parquet', index=False)\n",
    "\n",
    "display(df_combined.head(3))\n",
    "print(df_combined.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_combined.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "df_combined = pd.concat([df_dor, df_ori], ignore_index=True)\n",
    "csv_path = 'data/tidy_data_2_aed_model.csv'\n",
    "tar_path = 'data/tidy_data_2_aed_model.tar.xz'\n",
    "\n",
    "# Save CSV\n",
    "df_combined.to_csv(csv_path, index=False)\n",
    "\n",
    "# Compress to tar.xz\n",
    "with tarfile.open(tar_path, \"w:xz\") as tar:\n",
    "    tar.add(csv_path, arcname=os.path.basename(csv_path))\n",
    "\n",
    "# Remove the original CSV file\n",
    "os.remove(csv_path)\n",
    "display(df_combined.head(2))\n",
    "display(df_combined.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classical Metrics Report - Traditional Approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scikit_posthocs as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from IPython.display import display\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_parquet('data/tidy_data_2_aed_model.parquet')\n",
    "\n",
    "# --- Overall metrics ---\n",
    "y_true = df['answer'].to_numpy()\n",
    "y_pred = df['model_answer'].to_numpy()\n",
    "valid = ~pd.isna(y_true) & ~pd.isna(y_pred)\n",
    "y_true = y_true[valid]\n",
    "y_pred = y_pred[valid]\n",
    "\n",
    "overall_metrics = {\n",
    "    \"Accuracy\": [round(accuracy_score(y_true, y_pred), 4)],\n",
    "    \"Precision\": [round(precision_score(y_true, y_pred, average='weighted', zero_division=0), 4)],\n",
    "    \"Recall\": [round(recall_score(y_true, y_pred, average='weighted', zero_division=0), 4)],\n",
    "    \"F1-Score\": [round(f1_score(y_true, y_pred, average='weighted', zero_division=0), 4)]\n",
    "}\n",
    "\n",
    "print(\"ðŸ”¹ Overall Metrics:\")\n",
    "display(pd.DataFrame(overall_metrics))\n",
    "\n",
    "# --- Confusion matrix ---\n",
    "labels_all = sorted(list(set(y_true) | set(y_pred)))\n",
    "cm = pd.DataFrame(confusion_matrix(y_true, y_pred, labels=labels_all), index=labels_all, columns=labels_all)\n",
    "print(\"ðŸ”¹ Overall Confusion Matrix:\")\n",
    "display(cm)\n",
    "\n",
    "# --- Aggregated metrics by model ---\n",
    "print(\"ðŸ”¹ Aggregated Metrics by Model:\")\n",
    "\n",
    "model_metrics = []\n",
    "model_names = df['model'].unique()\n",
    "\n",
    "for model in model_names:\n",
    "    model_data = df[df['model'] == model]\n",
    "    yt = model_data['answer'].to_numpy()\n",
    "    yp = model_data['model_answer'].to_numpy()\n",
    "    valid = ~pd.isna(yt) & ~pd.isna(yp)\n",
    "    yt = yt[valid]\n",
    "    yp = yp[valid]\n",
    "\n",
    "    if len(yt) == 0:\n",
    "        continue\n",
    "\n",
    "    report = classification_report(yt, yp, output_dict=True, zero_division=0)\n",
    "\n",
    "    row = {\n",
    "        \"Model\": model,\n",
    "        \"Accuracy\": round(report.get(\"accuracy\", np.nan), 4),\n",
    "        \"Macro_Precision\": round(report[\"macro avg\"][\"precision\"], 4),\n",
    "        \"Macro_Recall\": round(report[\"macro avg\"][\"recall\"], 4),\n",
    "        \"Macro_F1\": round(report[\"macro avg\"][\"f1-score\"], 4),\n",
    "        \"Weighted_Precision\": round(report[\"weighted avg\"][\"precision\"], 4),\n",
    "        \"Weighted_Recall\": round(report[\"weighted avg\"][\"recall\"], 4),\n",
    "        \"Weighted_F1\": round(report[\"weighted avg\"][\"f1-score\"], 4)\n",
    "    }\n",
    "\n",
    "    model_metrics.append(row)\n",
    "\n",
    "df_summary = pd.DataFrame(model_metrics)\n",
    "display(df_summary)\n",
    "\n",
    "# --- F1 matrix per class ---\n",
    "print(\"ðŸ”¹ Class-level F1-Score Matrix:\")\n",
    "\n",
    "f1_matrix = []\n",
    "\n",
    "for label in labels_all:\n",
    "    f1_row = []\n",
    "    for model in model_names:\n",
    "        data = df[df['model'] == model]\n",
    "        yt = data['answer'].to_numpy()\n",
    "        yp = data['model_answer'].to_numpy()\n",
    "        valid = ~pd.isna(yt) & ~pd.isna(yp)\n",
    "        yt = yt[valid]\n",
    "        yp = yp[valid]\n",
    "\n",
    "        if len(yt) == 0:\n",
    "            f1_row.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        report = classification_report(yt, yp, output_dict=True, zero_division=0)\n",
    "        f1 = report.get(label, {}).get('f1-score', 0.0)\n",
    "        f1_row.append(round(f1, 4))\n",
    "\n",
    "    f1_matrix.append(f1_row)\n",
    "\n",
    "f1_scores = np.array(f1_matrix)\n",
    "df_f1_matrix = pd.DataFrame(f1_scores, index=labels_all, columns=model_names)\n",
    "display(df_f1_matrix)\n",
    "\n",
    "# --- Filter valid models ---\n",
    "f1_scores_clean = np.array(f1_scores)\n",
    "valid_columns = ~np.isnan(f1_scores_clean).any(axis=0)\n",
    "f1_scores_clean = f1_scores_clean[:, valid_columns]\n",
    "valid_model_names = np.array(model_names)[valid_columns]\n",
    "\n",
    "# --- Friedman test ---\n",
    "print(\"ðŸ”¹ Friedman Test on Class-level F1-Scores:\")\n",
    "stat, p = stats.friedmanchisquare(*f1_scores_clean)\n",
    "print(f\"  Ï‡Â² = {stat:.3f}, p = {p:.4f}\")\n",
    "\n",
    "# --- Nemenyi post-hoc test ---\n",
    "if p < 0.05:\n",
    "    print(\"ðŸ”¹ Nemenyi Post-hoc Test (p < 0.05):\")\n",
    "    nemenyi_result = sp.posthoc_nemenyi_friedman(f1_scores_clean)\n",
    "    display(nemenyi_result)\n",
    "\n",
    "# --- Dunn's Test with Benjamini-Hochberg correction ---\n",
    "if p < 0.05:\n",
    "    print(\"ðŸ”¹ Dunn's Test with Benjamini-Hochberg correction:\")\n",
    "    try:\n",
    "        dunn_result = sp.posthoc_dunn(f1_scores_clean, p_adjust='fdr_bh')  # Updated method\n",
    "        display(dunn_result)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in Dunn's Test: {e}\")\n",
    "\n",
    "# --- Holm-Bonferroni method ---\n",
    "if p < 0.05:\n",
    "    print(\"ðŸ”¹ Holm-Bonferroni method:\")\n",
    "    try:\n",
    "        holm_result = sp.posthoc_dunn(f1_scores_clean, p_adjust='holm')  # Use Dunn's test with Holm adjustment\n",
    "        display(holm_result)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in Holm-Bonferroni method: {e}\")\n",
    "\n",
    "print(model_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
