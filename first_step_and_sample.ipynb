{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "db_arc = pd.read_parquet(\"data/ARC_test/test-00000-of-00001.parquet\")\n",
    "db_mmlu = pd.read_parquet(\"data/MMLU_test/test-00000-of-00001.parquet\")\n",
    "db_hellaswag = pd.read_json(\"/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/frame-llm/data/hellaswag_test/hellaswag_val.jsonl\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(db_arc.head())\n",
    "print(db_arc.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(db_mmlu.head())\n",
    "print(db_mmlu.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(db_hellaswag.head())\n",
    "print(db_hellaswag.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_arc_tmp = db_arc.copy()\n",
    "db_mmlu_tmp = db_mmlu.copy()\n",
    "db_hellaswag_tmp = db_hellaswag.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_arc_final(db_arc_tmp):\n",
    "    def format_arc_question(row):\n",
    "        question = row['question']\n",
    "        choices = row['choices']['text']\n",
    "\n",
    "        choices_str = \"\\n\".join([f\"{chr(65 + i)}. {choice}\" for i, choice in enumerate(choices)])\n",
    "\n",
    "        return f\"Question: {question}\\nChoices:\\n{choices_str}\"\n",
    "\n",
    "    arc_final = db_arc_tmp.apply(lambda row: pd.Series({\n",
    "        'item': format_arc_question(row),\n",
    "        'answer': row['answerKey']\n",
    "    }), axis=1)\n",
    "    return arc_final\n",
    "\n",
    "arc_final = create_arc_final(db_arc_tmp)\n",
    "display(arc_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mmlu_final(db_mmlu_tmp):\n",
    "    def format_mmlu_question(row):\n",
    "        question = row['question']\n",
    "        choices = row['choices']\n",
    "\n",
    "        choices_str = \"\\n\".join([f\"{chr(65 + i)}. {choice}\" for i, choice in enumerate(choices)])\n",
    "\n",
    "        return f\"Question: {question}\\nChoices:\\n{choices_str}\"\n",
    "\n",
    "    mmlu_final = db_mmlu_tmp.apply(lambda row: pd.Series({\n",
    "        'item': format_mmlu_question(row),\n",
    "        'answer': chr(65 + int(row['answer']))\n",
    "    }), axis=1)\n",
    "    return mmlu_final\n",
    "\n",
    "mmlu_final = create_mmlu_final(db_mmlu_tmp)\n",
    "display(mmlu_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HellaSwag\n",
    "\n",
    "*   **ind**: dataset ID\n",
    "*   **activity\\_label**: The ActivityNet or WikiHow label for this example\n",
    "*   **context**: There are two formats. The full context is in `ctx`. When the context ends in an (incomplete) noun phrase, like for ActivityNet, this incomplete noun phrase is in `ctx_b`, and the context up until then is in `ctx_a`. This can be useful for models such as BERT that need the last sentence to be complete. However, it's never required. If `ctx_b` is nonempty, then `ctx` is the same thing as `ctx_a`, followed by a space, then `ctx_b`.\n",
    "*   **endings**: a list of 4 endings. The correct index is given by `label` (0,1,2, or 3)\n",
    "*   **split**: train, val, or test.\n",
    "*   **split\\_type**: `indomain` if the activity label is seen during training, else `zeroshot`\n",
    "*   **source\\_id**: Which video or WikiHow article this example came from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hellaswag_prompt_simple(db_hellaswag_tmp):\n",
    "    def format_hellaswag_item(row):\n",
    "        ctx = row['ctx']\n",
    "        endings = row['endings']\n",
    "        choices_str = \"\\n\".join([f\"{chr(65 + i)}. {ending}\" for i, ending in enumerate(endings)])\n",
    "\n",
    "        return f\"Context: {ctx}\\nOptions:\\n{choices_str}\"\n",
    "\n",
    "    formatted = db_hellaswag_tmp.apply(lambda row: pd.Series({\n",
    "        'item': format_hellaswag_item(row),\n",
    "        'answer': chr(65 + int(row['label']))\n",
    "    }), axis=1)\n",
    "\n",
    "    return formatted\n",
    "\n",
    "\n",
    "hellaswag_final = create_hellaswag_prompt_simple(db_hellaswag_tmp)\n",
    "display(hellaswag_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column indicating the source of the data\n",
    "mmlu_final['source'] = 'MMLU'\n",
    "mmlu_final = mmlu_final[['source', 'item', 'answer']]\n",
    "\n",
    "hellaswag_final['source'] = 'HellaSwag'\n",
    "hellaswag_final = hellaswag_final[['source', 'item', 'answer']]\n",
    "\n",
    "arc_final['source'] = 'ARC'\n",
    "arc_final = arc_final[['source', 'item', 'answer']]\n",
    "\n",
    "# Concatenate all dataframes\n",
    "all_final = pd.concat([mmlu_final, hellaswag_final, arc_final], ignore_index=True)\n",
    "all_final.to_csv('data/all_sources.csv', index=False)\n",
    "\n",
    "display(all_final.head())\n",
    "print(all_final.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First estimation n = itens x dataset with equal number of samples from each 'source' category\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "sample_n = 45 * 3 # n = itens x dataset\n",
    "\n",
    "# Print total and proportion for each source\n",
    "source_counts = all_final['source'].value_counts()\n",
    "source_proportions = all_final['source'].value_counts(normalize=True)\n",
    "print(\"Source counts:\\n\", source_counts)\n",
    "print(\"\\nSource proportions:\\n\", source_proportions)\n",
    "\n",
    "# Get the unique sources\n",
    "sources = all_final['source'].unique()\n",
    "n_sources = len(sources)\n",
    "\n",
    "# Calculate how many samples per source (as equal as possible)\n",
    "samples_per_source = sample_n // n_sources\n",
    "remainder = sample_n % n_sources\n",
    "\n",
    "# Sample equally from each source\n",
    "sampled_dfs = []\n",
    "for i, source in enumerate(sources):\n",
    "    n_samples = samples_per_source + (1 if i < remainder else 0)\n",
    "    df_source = all_final[all_final['source'] == source]\n",
    "    # If there are not enough samples in a source, take all available\n",
    "    n_samples = min(n_samples, len(df_source))\n",
    "    sampled_dfs.append(df_source.sample(n=n_samples, random_state=42))\n",
    "\n",
    "sample_all_final = pd.concat(sampled_dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "sample_all_final.to_csv('data/sample.csv', index=False)\n",
    "display(sample_all_final)\n",
    "print(sample_all_final['source'].value_counts())\n",
    "print(sample_all_final['source'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **First Step**\n",
    "## **Zero-Shot prompt engineering for Chain‐of‐Thought and JSON output for itens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sample\n",
    "df = pd.read_csv('data/sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "import anthropic\n",
    "\n",
    "# Specify the path to your .env file\n",
    "dotenv_path = \"/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/.env\"\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "# Access and store the environment variable\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "claude_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "xai_api_key = os.getenv(\"XAI_API_KEY\")\n",
    "deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "# Config client\n",
    "client_gpt = OpenAI(api_key=openai_api_key)\n",
    "client_claude = anthropic.Anthropic(api_key=claude_api_key)\n",
    "client_grok = OpenAI(api_key=xai_api_key, base_url=\"https://api.x.ai/v1\")\n",
    "client_ds = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LLM output must be a JSON containing: step by step solution, answer\n",
    "models = {\n",
    "    'model_gpt_4o_mini': {'model': 'gpt-4o-mini', 'client': client_gpt},\n",
    "    'model_gpt_41_nano': {'model': 'gpt-4.1-nano', 'client': client_gpt},\n",
    "    'model_claude_35_haiku': {'model': 'claude-3-5-haiku-latest', 'client': client_claude},\n",
    "    'model_grok_3_mini_beta': {'model': 'grok-3-mini-beta', 'client': client_grok},\n",
    "    'model_ds_v3': {'model': 'deepseek-chat', 'client': client_ds}\n",
    "}\n",
    "\n",
    "def create_prompt_from_row(row, type):\n",
    "    \"\"\"\n",
    "    Create a prompt for an LLM to solve a multiple-choice question, \n",
    "    encouraging a divide and conquer approach. Output must be valid JSON.\n",
    "    \"\"\"\n",
    "\n",
    "    if type == 'cot':\n",
    "        prompt = (\n",
    "            \"You are an expert at solving multiple-choice questions. \"\n",
    "            \"Read the following question and its options carefully:\\n\"\n",
    "            f\"{row['item']}\\n\\n\"\n",
    "            \"To arrive at the solution, break down the problem into smaller, manageable stages wherever possible. \"\n",
    "            \"Enumerate and show your reasoning step by step (1., 2., ...), then select the answer you believe is correct (A, B, C or D).\\n\\n\"\n",
    "            \"Your output must be a single valid JSON object with these fields (no extra text or markdown):\\n\"\n",
    "            \"```json\\n\"\n",
    "            \"{\\n\"\n",
    "            '  \"CoT\": \"<detailed reasoning here>\",\\n'\n",
    "            '  \"answer\": \"<A, B, C or D>\",\\n'\n",
    "            '  \"justification\": \"<brief explanation of your choice>\"\\n'\n",
    "            \"}\\n\"\n",
    "            \"```\\n\"\n",
    "            \"Ensure the JSON is parseable by a standard JSON parser (double quotes for keys, no trailing commas).\"\n",
    "        )\n",
    "    elif type == 'naive':\n",
    "        prompt = (\n",
    "            \"Read the following question and its options:\\n\"\n",
    "            f\"{row['item']}\\n\\n\"\n",
    "            \"Select the answer you believe is correct (A, B, C or D).\\n\\n\"\n",
    "            \"Your output must be a single valid JSON object with these fields (no extra text or markdown):\\n\"\n",
    "            \"```json\\n\"\n",
    "            \"{\\n\"\n",
    "            '  \"answer\": \"<A, B, C or D>\",\\n'\n",
    "            '  \"justification\": \"<brief explanation of your choice>\"\\n'\n",
    "            \"}\\n\"\n",
    "            \"```\\n\"\n",
    "            \"Ensure the JSON is parseable by a standard JSON parser (double quotes for keys, no trailing commas).\"\n",
    "        )\n",
    "    elif type == 'adversarial':\n",
    "        prompt = (\n",
    "            \"You are an expert at solving multiple-choice questions, but your reasoning might be flawed. \"\n",
    "            \"Read the following question and its options carefully. Consider that some subtle aspects of the question might be designed to mislead you:\\n\\n\"\n",
    "            f\"{row['item']}\\n\\n\"\n",
    "            \"Also, be aware that there might be an alternative interpretation or a hidden assumption within the question that could change the correct answer.\\n\\n\"\n",
    "            \"If you determine that none of A, B, C or D is fully correct, choose E and provide your own alternative answer.\\n\\n\"\n",
    "            \"To arrive at the solution, meticulously examine each step of your reasoning and question every assumption you make. \"\n",
    "            \"Enumerate and show your reasoning step by step (1., 2., ...), then select the answer you believe is correct (A, B, C, D or E).\\n\\n\"\n",
    "            \"Your output must be a single valid JSON object with these fields (no extra text or markdown):\\n\"\n",
    "            \"```json\\n\"\n",
    "            \"{\\n\"\n",
    "            '  \"CoT\": \"<detailed reasoning here, questioning your own assumptions>\",\\n'\n",
    "            '  \"answer\": \"<A, B, C, D or E>\",\\n'\n",
    "            '  \"alternative_answer\": \"<text of your E option, or null if not used>\",\\n'\n",
    "            '  \"justification\": \"<brief explanation of your choice, highlighting potential doubts or uncertainties>\"\\n'\n",
    "            \"}\\n\"\n",
    "            \"```\\n\"\n",
    "            \"Ensure the JSON is parseable by a standard JSON parser (double quotes for keys, no trailing commas).\"\n",
    "        )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Example usage for i-th row:\n",
    "i = 0\n",
    "print(create_prompt_from_row(row=df.iloc[i], type='cot')+'\\n\\n')\n",
    "print(create_prompt_from_row(row=df.iloc[i], type='naive')+'\\n\\n')\n",
    "print(create_prompt_from_row(row=df.iloc[i], type='adversarial')+'\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_one(model_name, prompt):\n",
    "    \"\"\"\n",
    "    Queries the specified LLM with the given prompt and returns the generated text.\n",
    "\n",
    "    The function retrieves the model configuration from the 'models' dictionary,\n",
    "    then uses the appropriate client (OpenAI, Gemini or Anthropic) to generate\n",
    "    a response.  The generated text is extracted from the response object and returned.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the model to use (key in the 'models' dictionary).\n",
    "        prompt (str): The prompt to send to the model.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text from the model.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the model_name is not found in the 'models' dictionary,\n",
    "                    or if an unsupported client type is encountered.\n",
    "    \"\"\"\n",
    "    model_config = models.get(model_name)\n",
    "    if not model_config:\n",
    "        raise ValueError(f\"Model '{model_name}' not found in the 'models' dictionary.\")\n",
    "\n",
    "    client = model_config['client']\n",
    "    model = model_config['model']\n",
    "\n",
    "    if isinstance(client, OpenAI):\n",
    "        response = client.chat.completions.create(model=model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        generated_text = response.choices[0].message.content\n",
    "    # elif isinstance(client, genai.Client):\n",
    "    #     response = client.models.generate_content(model=model, contents=prompt)\n",
    "    #     generated_text = response.text\n",
    "    elif isinstance(client, anthropic.Anthropic):\n",
    "        response = client.messages.create(model=model, max_tokens=1000, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        generated_text = response.content[0].text\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported client type.\")\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "df_cp = df.copy()\n",
    "prompt_types = ['cot', 'naive', 'adversarial']\n",
    "\n",
    "def process_model(model_name, df_model):\n",
    "    results = []\n",
    "    print(model_name)\n",
    "    for prompt_type in prompt_types:\n",
    "        for i in tqdm(range(len(df_model)), desc=f\"Processing {model_name} - {prompt_type}\"):\n",
    "            for r in range(5):  # Repeat the call 5 times\n",
    "                prompt = create_prompt_from_row(row=df_model.iloc[i], type=prompt_type)\n",
    "                output = step_one(model_name, prompt)\n",
    "                results.append({\n",
    "                    'source': df_model.iloc[i]['source'],\n",
    "                    'item': df_model.iloc[i]['item'],\n",
    "                    'r': r,\n",
    "                    'model': model_name,\n",
    "                    'prompt_type': prompt_type,\n",
    "                    'output': output\n",
    "                })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Prepare model-specific dataframes\n",
    "model_dfs = {}\n",
    "for model_name in models.keys():\n",
    "    model_dfs[model_name] = df_cp.copy()  # Create a copy for each model\n",
    "\n",
    "# Parallelize the processing of each model\n",
    "results = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=len(models)) as executor:\n",
    "    futures = {executor.submit(process_model, model_name, model_dfs[model_name]): model_name for model_name in models.keys()}\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        model_name = futures[future]\n",
    "        try:\n",
    "            model_results = future.result()\n",
    "            results.append(model_results)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model_name}: {e}\")\n",
    "\n",
    "# Concatenate results from all models\n",
    "results_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Merge results back into the original dataframe\n",
    "df_cp = pd.merge(df_cp, results_df, on=['source', 'item'], how='left')\n",
    "\n",
    "# Save the updated dataframe\n",
    "df_cp.to_csv('data/res_step_one.csv', index=False)\n",
    "print(df_cp.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate results from all models\n",
    "results_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Merge results back into the original dataframe\n",
    "df_cp = pd.merge(df_cp, results_df, on=['source', 'item'], how='left')\n",
    "\n",
    "# Save the updated dataframe\n",
    "df_cp.to_csv('data/res_step_one.csv', index=False)\n",
    "print(df_cp.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cp = df_cp[df_cp['model'] != 'model_deepseek_reasoner']\n",
    "df_cp.to_csv('data/res_step_one_clean.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
