{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2945690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read responses (1 row = respondent)\n",
    "df_resp = pd.read_excel(\"data/Human Evaluation_ DoR and ORI (135 items + ACE) (respostas).xlsx\", dtype=object)\n",
    "sample = pd.read_csv(\"out/human_eval_sample.csv\")\n",
    "\n",
    "# Drop timestamp\n",
    "ts_col = \"Carimbo de data/hora\"\n",
    "if ts_col in df_resp.columns:\n",
    "    df_resp = df_resp.drop(columns=[ts_col])\n",
    "\n",
    "# Anonymize email column\n",
    "email_col = \"Endereço de e-mail\"\n",
    "if email_col not in df_resp.columns:\n",
    "    raise KeyError(f\"Expected column '{email_col}' in the responses export\")\n",
    "\n",
    "emails = (\n",
    "    df_resp[email_col]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .replace({\"\": np.nan})\n",
    "    .dropna()\n",
    "    .unique()\n",
    ")\n",
    "emails = sorted(emails)\n",
    "if len(emails) > 1000:\n",
    "    raise ValueError(f\"Too many unique emails to map to 3 digits: {len(emails)}\")\n",
    "\n",
    "email_to_id = {e: f\"{i:03d}\" for i, e in enumerate(emails)}\n",
    "df_resp = df_resp.rename(columns={email_col: \"respondent_id\"})\n",
    "df_resp[\"respondent_id\"] = (\n",
    "    df_resp[\"respondent_id\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .map(email_to_id)\n",
    ")\n",
    "if df_resp[\"respondent_id\"].isna().any():\n",
    "    raise ValueError(\"Some respondents have missing/unknown email values; cannot anonymize reliably\")\n",
    "\n",
    "# Remaining columns are DoR/ORI ratings in pairs\n",
    "rating_cols = [c for c in df_resp.columns if c != \"respondent_id\"]\n",
    "df_long = df_resp.melt(\n",
    "    id_vars=[\"respondent_id\"],\n",
    "    value_vars=rating_cols,\n",
    "    var_name=\"question\",\n",
    "    value_name=\"rating\",\n",
    ")\n",
    "\n",
    "def _parse_item_id(q: str) -> int:\n",
    "    m = re.search(r\"(\\d{3})\", str(q))\n",
    "    if not m:\n",
    "        raise ValueError(f\"Could not parse 3-digit item id from: {q}\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "def _parse_metric(q: str) -> str:\n",
    "    q = str(q)\n",
    "    m = re.search(r\"\\[(DoR|ORI)\\]\\s*$\", q, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).upper()\n",
    "    if \"DoR\" in q:\n",
    "        return \"DOR\"\n",
    "    if \"ORI\" in q:\n",
    "        return \"ORI\"\n",
    "    raise ValueError(f\"Could not detect metric (DoR/ORI) from: {q}\")\n",
    "\n",
    "df_long[\"item_id\"] = df_long[\"question\"].map(_parse_item_id)\n",
    "df_long[\"metric\"] = df_long[\"question\"].map(_parse_metric)\n",
    "\n",
    "# Keep as numeric (NaN for blanks)\n",
    "df_long[\"rating\"] = pd.to_numeric(df_long[\"rating\"], errors=\"coerce\")\n",
    "\n",
    "# Validate uniqueness per respondent/item/metric\n",
    "dup = (\n",
    "    df_long.groupby([\"respondent_id\", \"item_id\", \"metric\"], dropna=False)\n",
    "    .size()\n",
    "    .reset_index(name=\"n\")\n",
    ")\n",
    "dup = dup[dup[\"n\"] > 1]\n",
    "if len(dup):\n",
    "    raise ValueError(f\"Duplicate ratings detected:\\n{dup.head(20)}\")\n",
    "\n",
    "# Pivot to 1 row per item, 2 cols per respondent: gr_dor_human_XXX, gr_ori_human_XXX\n",
    "df_wide = df_long.pivot_table(\n",
    "    index=\"item_id\",\n",
    "    columns=[\"respondent_id\", \"metric\"],\n",
    "    values=\"rating\",\n",
    "    aggfunc=\"first\",\n",
    ")\n",
    "\n",
    "def _col_name(respondent_id: str, metric: str) -> str:\n",
    "    metric = metric.lower()\n",
    "    return f\"gr_{metric}_human_{respondent_id}\"\n",
    "\n",
    "df_wide.columns = [_col_name(rid, metric) for (rid, metric) in df_wide.columns.to_list()]\n",
    "df_wide = df_wide.sort_index().reset_index()\n",
    "\n",
    "# Optional: attach item metadata from the generated sample (Item 001.. = row order in the CSV)\n",
    "if \"sample\" in globals():\n",
    "    df_items = sample.copy()\n",
    "    df_items[\"item_id\"] = np.arange(1, len(df_items) + 1)\n",
    "    df_human = df_items.merge(df_wide, on=\"item_id\", how=\"left\")\n",
    "else:\n",
    "    df_human = df_wide\n",
    "\n",
    "print(\"Respondents:\", len(emails))\n",
    "print(\"Items:\", df_human[\"item_id\"].nunique())\n",
    "display(df_human.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ba95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import kendalltau, chi2_contingency\n",
    "\n",
    "if \"df_human\" not in globals():\n",
    "    raise NameError(\"Expected df_human from the previous chunk\")\n",
    "\n",
    "MODEL_PARQUET = \"data/tidy_data_2_aed_model.parquet\"\n",
    "df_model = pd.read_parquet(MODEL_PARQUET)\n",
    "\n",
    "# Join human ratings to model-judge ratings\n",
    "if \"_orig_index\" in df_human.columns:\n",
    "    if \"_orig_index\" not in df_model.columns:\n",
    "        df_model = df_model.reset_index().rename(columns={\"index\": \"_orig_index\"})\n",
    "    df_align = df_human.merge(df_model, on=\"_orig_index\", how=\"left\", suffixes=(\"\", \"_model\"))\n",
    "elif \"item_id\" in df_model.columns:\n",
    "    df_align = df_human.merge(df_model, on=\"item_id\", how=\"left\", suffixes=(\"\", \"_model\"))\n",
    "else:\n",
    "    raise KeyError(\"Could not join df_human to df_model (need '_orig_index' or 'item_id' in df_model)\")\n",
    "\n",
    "\n",
    "def _cols(prefix: str, df: pd.DataFrame) -> list[str]:\n",
    "    return [c for c in df.columns if c.startswith(prefix)]\n",
    "\n",
    "\n",
    "def _discretize_scores(s: pd.Series) -> pd.Series:\n",
    "    bins = [-np.inf, 1, 3, 5, 7, 9, 10]\n",
    "    labels = [\"[0,1]\", \"(1,3]\", \"(3,5]\", \"(5,7]\", \"(7,9]\", \"(9,10]\"]\n",
    "    return pd.cut(pd.to_numeric(s, errors=\"coerce\"), bins=bins, labels=labels, right=True, include_lowest=True)\n",
    "\n",
    "\n",
    "def _discretize_scores_bins(s: pd.Series, bins: list[float], labels: list[str]) -> pd.Series:\n",
    "    if len(bins) != len(labels) + 1:\n",
    "        raise ValueError(\"bins must have len(labels)+1\")\n",
    "    return pd.cut(\n",
    "        pd.to_numeric(s, errors=\"coerce\"),\n",
    "        bins=bins,\n",
    "        labels=labels,\n",
    "        right=True,\n",
    "        include_lowest=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# Alternative discretizations for Krippendorff's α (ordinal)\n",
    "ALPHA_DISCRETIZATION_SCENARIOS = {\n",
    "    \"2_bins_0_5__6_10\": {\n",
    "        \"bins\": [-np.inf, 5, 10],\n",
    "        \"labels\": [\"0-5\", \"6-10\"],\n",
    "    },\n",
    "    \"3_bins_0_3__4_6__7_10\": {\n",
    "        \"bins\": [-np.inf, 3, 6, 10],\n",
    "        \"labels\": [\"0-3\", \"4-6\", \"7-10\"],\n",
    "    },\n",
    "    \"6_bins_0_1__2_3__3_5__6_7__8_9__9_10\": {\n",
    "        \"bins\": [-np.inf, 1, 3, 5, 7, 9, 10],\n",
    "        \"labels\": [\"0-1\", \"2-3\", \"3-5\", \"6-7\", \"8-9\", \"9-10\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def _weighted_kappa(a: pd.Series, b: pd.Series, n_cats: int, weights: str = \"quadratic\") -> float:\n",
    "    \"\"\"Cohen's weighted kappa for ordinal categories encoded as 0..n_cats-1 (NaN allowed).\"\"\"\n",
    "    a = pd.to_numeric(a, errors=\"coerce\")\n",
    "    b = pd.to_numeric(b, errors=\"coerce\")\n",
    "    m = a.notna() & b.notna()\n",
    "    if int(m.sum()) == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    ai = a[m].astype(int).to_numpy()\n",
    "    bi = b[m].astype(int).to_numpy()\n",
    "\n",
    "    k = int(n_cats)\n",
    "    if k < 2:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    O = np.zeros((k, k), dtype=float)\n",
    "    for x, y in zip(ai, bi):\n",
    "        if 0 <= x < k and 0 <= y < k:\n",
    "            O[x, y] += 1.0\n",
    "\n",
    "    N = O.sum()\n",
    "    if N == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    row = O.sum(axis=1)\n",
    "    col = O.sum(axis=0)\n",
    "    E = np.outer(row, col) / N\n",
    "\n",
    "    idx = np.arange(k)\n",
    "    if weights == \"quadratic\":\n",
    "        W = ((idx[:, None] - idx[None, :]) / (k - 1)) ** 2\n",
    "    elif weights == \"linear\":\n",
    "        W = np.abs(idx[:, None] - idx[None, :]) / (k - 1)\n",
    "    else:\n",
    "        raise ValueError(\"weights must be 'linear' or 'quadratic'\")\n",
    "\n",
    "    num = float((W * O).sum())\n",
    "    den = float((W * E).sum())\n",
    "    if den == 0:\n",
    "        return float(\"nan\")\n",
    "    return float(1 - (num / den))\n",
    "\n",
    "\n",
    "def _cramers_v(x: pd.Series, y: pd.Series) -> float:\n",
    "    tbl = pd.crosstab(x, y)\n",
    "    if tbl.size == 0:\n",
    "        return float(\"nan\")\n",
    "    chi2, _, _, _ = chi2_contingency(tbl)\n",
    "    n = tbl.to_numpy().sum()\n",
    "    r, k = tbl.shape\n",
    "    denom = n * min(r - 1, k - 1)\n",
    "    return float(np.sqrt(chi2 / denom)) if denom > 0 else float(\"nan\")\n",
    "\n",
    "\n",
    "def _cramers_v_p(x: pd.Series, y: pd.Series) -> tuple[float, float]:\n",
    "    tbl = pd.crosstab(x, y)\n",
    "    if tbl.size == 0:\n",
    "        return float(\"nan\"), float(\"nan\")\n",
    "    chi2, p, _, _ = chi2_contingency(tbl)\n",
    "    n = tbl.to_numpy().sum()\n",
    "    r, k = tbl.shape\n",
    "    denom = n * min(r - 1, k - 1)\n",
    "    v = float(np.sqrt(chi2 / denom)) if denom > 0 else float(\"nan\")\n",
    "    return v, float(p)\n",
    "\n",
    "\n",
    "def krippendorff_alpha(values: np.ndarray, level: str = \"interval\") -> float:\n",
    "    \"\"\"Krippendorff's alpha for 2D array (units x raters), NaN = missing.\"\"\"\n",
    "    if values.ndim != 2:\n",
    "        raise ValueError(\"values must be 2D (units x raters)\")\n",
    "\n",
    "    vals = values.astype(float)\n",
    "    cats = np.unique(vals[~np.isnan(vals)])\n",
    "    if cats.size == 0:\n",
    "        return float(\"nan\")\n",
    "    cats = np.sort(cats)\n",
    "    cat_to_i = {c: i for i, c in enumerate(cats)}\n",
    "\n",
    "    O = np.zeros((len(cats), len(cats)), dtype=float)\n",
    "\n",
    "    for row in vals:\n",
    "        row = row[~np.isnan(row)]\n",
    "        m = len(row)\n",
    "        if m < 2:\n",
    "            continue\n",
    "        u, cnt = np.unique(row, return_counts=True)\n",
    "        counts = {u_i: int(c_i) for u_i, c_i in zip(u, cnt)}\n",
    "\n",
    "        # Diagonal\n",
    "        for c, n_c in counts.items():\n",
    "            i = cat_to_i[c]\n",
    "            O[i, i] += n_c * (n_c - 1) / (m - 1)\n",
    "\n",
    "        # Off-diagonal\n",
    "        for c, n_c in counts.items():\n",
    "            i = cat_to_i[c]\n",
    "            for k, n_k in counts.items():\n",
    "                if c == k:\n",
    "                    continue\n",
    "                j = cat_to_i[k]\n",
    "                O[i, j] += n_c * n_k / (m - 1)\n",
    "\n",
    "    n_c = O.sum(axis=1)\n",
    "    N = n_c.sum()\n",
    "    if N <= 1:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    # Expected coincidences\n",
    "    E = np.outer(n_c, n_c) / (N - 1)\n",
    "    np.fill_diagonal(E, n_c * (n_c - 1) / (N - 1))\n",
    "\n",
    "    # Distances\n",
    "    if level == \"interval\":\n",
    "        D = (cats[:, None] - cats[None, :]) ** 2\n",
    "    elif level == \"ordinal\":\n",
    "        # Krippendorff ordinal distance uses category totals n_c\n",
    "        D = np.zeros_like(O)\n",
    "        for i in range(len(cats)):\n",
    "            for j in range(len(cats)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                lo, hi = (i, j) if i < j else (j, i)\n",
    "                s = n_c[lo : hi + 1].sum() - (n_c[lo] + n_c[hi]) / 2\n",
    "                D[i, j] = s ** 2\n",
    "    else:\n",
    "        raise ValueError(\"level must be 'interval' or 'ordinal'\")\n",
    "\n",
    "    Do = float((O * D).sum() / O.sum()) if O.sum() else float(\"nan\")\n",
    "    De = float((E * D).sum() / E.sum()) if E.sum() else float(\"nan\")\n",
    "    if not np.isfinite(Do) or not np.isfinite(De) or De == 0:\n",
    "        return float(\"nan\")\n",
    "    return float(1 - (Do / De))\n",
    "\n",
    "\n",
    "def _summarize(metric: str) -> dict:\n",
    "    human_cols = _cols(f\"gr_{metric}_human_\", df_align)\n",
    "    if not human_cols:\n",
    "        raise KeyError(f\"No human columns found for metric '{metric}'\")\n",
    "\n",
    "    # Model-judge columns: start with gr_{metric}_ but not human and not precomputed medians\n",
    "    candidates = [\n",
    "        c for c in _cols(f\"gr_{metric}_\", df_align)\n",
    "        if (\"human_\" not in c)\n",
    "        and (\"outlier\" not in c)\n",
    "        and (not c.endswith(\"_median\"))\n",
    "    ]\n",
    "    judge_cols = [\n",
    "        c for c in candidates\n",
    "        if not pd.api.types.is_bool_dtype(df_align[c])\n",
    "    ]\n",
    "    if not judge_cols:\n",
    "        raise KeyError(f\"No model-judge columns found for metric '{metric}'\")\n",
    "\n",
    "    human_med = df_align[human_cols].median(axis=1, skipna=True)\n",
    "    judge_med = df_align[judge_cols].median(axis=1, skipna=True)\n",
    "\n",
    "    valid = human_med.notna() & judge_med.notna()\n",
    "    try:\n",
    "        tau, p = kendalltau(human_med[valid], judge_med[valid], variant=\"b\")\n",
    "    except TypeError:\n",
    "        tau, p = kendalltau(human_med[valid], judge_med[valid])\n",
    "\n",
    "    # Krippendorff alpha (inter-rater reliability)\n",
    "    human_mat = df_align[human_cols].apply(pd.to_numeric, errors=\"coerce\").to_numpy()\n",
    "    judge_mat = df_align[judge_cols].apply(pd.to_numeric, errors=\"coerce\").to_numpy()\n",
    "    all_mat = df_align[human_cols + judge_cols].apply(pd.to_numeric, errors=\"coerce\").to_numpy()\n",
    "\n",
    "    alpha_human_interval = krippendorff_alpha(human_mat, level=\"interval\")\n",
    "    alpha_judge_interval = krippendorff_alpha(judge_mat, level=\"interval\")\n",
    "    alpha_all_interval = krippendorff_alpha(all_mat, level=\"interval\")\n",
    "\n",
    "    # Agreement between median ensembles (humans vs LLM judges) as 2-rater alpha\n",
    "    ensemble_mat = np.column_stack([\n",
    "        pd.to_numeric(human_med, errors=\"coerce\").to_numpy(),\n",
    "        pd.to_numeric(judge_med, errors=\"coerce\").to_numpy(),\n",
    "    ])\n",
    "    alpha_ensemble_interval = krippendorff_alpha(ensemble_mat, level=\"interval\")\n",
    "\n",
    "    # Ordinal alpha on discretized rubric intervals (0..5 ordered)\n",
    "    human_disc = df_align[human_cols].apply(_discretize_scores).apply(lambda c: c.cat.codes.replace(-1, np.nan))\n",
    "    judge_disc = df_align[judge_cols].apply(_discretize_scores).apply(lambda c: c.cat.codes.replace(-1, np.nan))\n",
    "    all_disc = df_align[human_cols + judge_cols].apply(_discretize_scores).apply(lambda c: c.cat.codes.replace(-1, np.nan))\n",
    "    alpha_human_ordinal = krippendorff_alpha(human_disc.to_numpy(), level=\"ordinal\")\n",
    "    alpha_judge_ordinal = krippendorff_alpha(judge_disc.to_numpy(), level=\"ordinal\")\n",
    "    alpha_all_ordinal = krippendorff_alpha(all_disc.to_numpy(), level=\"ordinal\")\n",
    "\n",
    "    human_med_disc = _discretize_scores(human_med).cat.codes.replace(-1, np.nan)\n",
    "    judge_med_disc = _discretize_scores(judge_med).cat.codes.replace(-1, np.nan)\n",
    "    kappa_weighted_rubric = _weighted_kappa(human_med_disc, judge_med_disc, n_cats=6, weights=\"quadratic\")\n",
    "    alpha_ensemble_ordinal = krippendorff_alpha(\n",
    "        np.column_stack([human_med_disc.to_numpy(), judge_med_disc.to_numpy()]),\n",
    "        level=\"ordinal\",\n",
    "    )\n",
    "\n",
    "    # Ordinal alpha under alternative discretizations\n",
    "    alpha_scenarios = {}\n",
    "    for name, spec in ALPHA_DISCRETIZATION_SCENARIOS.items():\n",
    "        bins = spec[\"bins\"]\n",
    "        labels = spec[\"labels\"]\n",
    "        h_disc = df_align[human_cols].apply(lambda col: _discretize_scores_bins(col, bins=bins, labels=labels))\n",
    "        j_disc = df_align[judge_cols].apply(lambda col: _discretize_scores_bins(col, bins=bins, labels=labels))\n",
    "        a_disc = df_align[human_cols + judge_cols].apply(lambda col: _discretize_scores_bins(col, bins=bins, labels=labels))\n",
    "        h_codes = h_disc.apply(lambda c: c.cat.codes.replace(-1, np.nan))\n",
    "        j_codes = j_disc.apply(lambda c: c.cat.codes.replace(-1, np.nan))\n",
    "        a_codes = a_disc.apply(lambda c: c.cat.codes.replace(-1, np.nan))\n",
    "        alpha_scenarios[f\"alpha_human_ordinal__{name}\"] = krippendorff_alpha(h_codes.to_numpy(), level=\"ordinal\")\n",
    "        alpha_scenarios[f\"alpha_judge_ordinal__{name}\"] = krippendorff_alpha(j_codes.to_numpy(), level=\"ordinal\")\n",
    "        alpha_scenarios[f\"alpha_all_ordinal__{name}\"] = krippendorff_alpha(a_codes.to_numpy(), level=\"ordinal\")\n",
    "\n",
    "        hm = _discretize_scores_bins(human_med, bins=bins, labels=labels).cat.codes.replace(-1, np.nan)\n",
    "        jm = _discretize_scores_bins(judge_med, bins=bins, labels=labels).cat.codes.replace(-1, np.nan)\n",
    "        alpha_scenarios[f\"kappa_weighted__{name}\"] = _weighted_kappa(hm, jm, n_cats=len(labels), weights=\"quadratic\")\n",
    "        v_s, p_s = _cramers_v_p(\n",
    "            _discretize_scores_bins(human_med, bins=bins, labels=labels),\n",
    "            _discretize_scores_bins(judge_med, bins=bins, labels=labels),\n",
    "        )\n",
    "        alpha_scenarios[f\"cramers_v__{name}\"] = float(v_s)\n",
    "        alpha_scenarios[f\"cramers_p__{name}\"] = float(p_s)\n",
    "        alpha_scenarios[f\"alpha_ensemble_ordinal__{name}\"] = krippendorff_alpha(\n",
    "            np.column_stack([hm.to_numpy(), jm.to_numpy()]),\n",
    "            level=\"ordinal\",\n",
    "        )\n",
    "\n",
    "    # Cramer's V between discretized medians (with p-value)\n",
    "    v, v_p = _cramers_v_p(_discretize_scores(human_med), _discretize_scores(judge_med))\n",
    "\n",
    "    return {\n",
    "        \"metric\": metric.upper(),\n",
    "        \"n_items\": int(valid.sum()),\n",
    "        \"kendall_tau_b\": float(tau),\n",
    "        \"kendall_p\": float(p),\n",
    "        \"alpha_human_interval\": float(alpha_human_interval),\n",
    "        \"alpha_judge_interval\": float(alpha_judge_interval),\n",
    "        \"alpha_all_interval\": float(alpha_all_interval),\n",
    "        \"alpha_ensemble_interval\": float(alpha_ensemble_interval),\n",
    "        \"alpha_human_ordinal\": float(alpha_human_ordinal),\n",
    "        \"alpha_judge_ordinal\": float(alpha_judge_ordinal),\n",
    "        \"alpha_all_ordinal\": float(alpha_all_ordinal),\n",
    "        \"alpha_ensemble_ordinal\": float(alpha_ensemble_ordinal),\n",
    "        \"kappa_weighted_rubric\": float(kappa_weighted_rubric),\n",
    "        **{k: float(v) for k, v in alpha_scenarios.items()},\n",
    "        \"cramers_v_discretized_medians\": float(v),\n",
    "        \"cramers_p_discretized_medians\": float(v_p),\n",
    "        \"n_humans\": len(human_cols),\n",
    "        \"n_model_judges\": len(judge_cols),\n",
    "    }\n",
    "\n",
    "\n",
    "summary = pd.DataFrame([_summarize(\"dor\"), _summarize(\"ori\")])\n",
    "\n",
    "# Structured α table for printing\n",
    "def _alpha_row(metric_row: pd.Series, scale: str, kind: str, suffix: str = \"\") -> dict:\n",
    "    suf = f\"__{suffix}\" if suffix else \"\"\n",
    "    if kind == \"interval\":\n",
    "        return {\n",
    "            \"metric\": metric_row[\"metric\"],\n",
    "            \"kind\": \"interval\",\n",
    "            \"scale\": scale,\n",
    "            \"alpha_humans\": metric_row[\"alpha_human_interval\"],\n",
    "            \"alpha_llms\": metric_row[\"alpha_judge_interval\"],\n",
    "            \"alpha_all\": metric_row[\"alpha_all_interval\"],\n",
    "            \"alpha_ensemble_medians\": metric_row[\"alpha_ensemble_interval\"],\n",
    "            \"kappa_weighted\": np.nan,\n",
    "            \"cramers_v\": np.nan,\n",
    "            \"cramers_p\": np.nan,\n",
    "        }\n",
    "\n",
    "    # ordinal\n",
    "    return {\n",
    "        \"metric\": metric_row[\"metric\"],\n",
    "        \"kind\": \"ordinal\",\n",
    "        \"scale\": scale,\n",
    "        \"alpha_humans\": metric_row[f\"alpha_human_ordinal{suf}\"] if suf else metric_row[\"alpha_human_ordinal\"],\n",
    "        \"alpha_llms\": metric_row[f\"alpha_judge_ordinal{suf}\"] if suf else metric_row[\"alpha_judge_ordinal\"],\n",
    "        \"alpha_all\": metric_row[f\"alpha_all_ordinal{suf}\"] if suf else metric_row[\"alpha_all_ordinal\"],\n",
    "        \"alpha_ensemble_medians\": metric_row[f\"alpha_ensemble_ordinal{suf}\"] if suf else metric_row[\"alpha_ensemble_ordinal\"],\n",
    "        \"kappa_weighted\": metric_row[f\"kappa_weighted{suf}\"] if suf else metric_row[\"kappa_weighted_rubric\"],\n",
    "        \"cramers_v\": metric_row[f\"cramers_v{suf}\"] if suf else metric_row[\"cramers_v_discretized_medians\"],\n",
    "        \"cramers_p\": metric_row[f\"cramers_p{suf}\"] if suf else metric_row[\"cramers_p_discretized_medians\"],\n",
    "    }\n",
    "\n",
    "\n",
    "alpha_rows = []\n",
    "for _, r in summary.iterrows():\n",
    "    alpha_rows.append(_alpha_row(r, scale=\"raw_0_10\", kind=\"interval\"))\n",
    "    alpha_rows.append(_alpha_row(r, scale=\"rubric_6_bins\", kind=\"ordinal\"))\n",
    "    for name in ALPHA_DISCRETIZATION_SCENARIOS.keys():\n",
    "        alpha_rows.append(_alpha_row(r, scale=name, kind=\"ordinal\", suffix=name))\n",
    "\n",
    "alpha_table = pd.DataFrame(alpha_rows)\n",
    "\n",
    "# Order rows: interval first, then ordinal (rubric first, then scenarios)\n",
    "scale_order = [\"raw_0_10\", \"rubric_6_bins\"] + list(ALPHA_DISCRETIZATION_SCENARIOS.keys())\n",
    "alpha_table[\"_scale_order\"] = alpha_table[\"scale\"].map({s: i for i, s in enumerate(scale_order)})\n",
    "alpha_table[\"_kind_order\"] = alpha_table[\"kind\"].map({\"interval\": 0, \"ordinal\": 1})\n",
    "alpha_table = alpha_table.sort_values([\"metric\", \"_kind_order\", \"_scale_order\"]).drop(columns=[\"_kind_order\", \"_scale_order\"]) \n",
    "\n",
    "# Pretty print\n",
    "alpha_table_fmt = alpha_table.copy()\n",
    "for c in [\"alpha_humans\", \"alpha_llms\", \"alpha_all\", \"alpha_ensemble_medians\", \"kappa_weighted\", \"cramers_v\"]:\n",
    "    alpha_table_fmt[c] = pd.to_numeric(alpha_table_fmt[c], errors=\"coerce\").round(3)\n",
    "alpha_table_fmt[\"cramers_p\"] = pd.to_numeric(alpha_table_fmt[\"cramers_p\"], errors=\"coerce\")\n",
    "\n",
    "print(\"=\")\n",
    "print(\"Agreement summary — Krippendorff’s α, weighted κ (bins), Cramér’s V (+p)\")\n",
    "print(alpha_table_fmt.to_string(index=False))\n",
    "\n",
    "# -----------------------------\n",
    "# Kendall τ_b table (humans vs LLMs)\n",
    "# -----------------------------\n",
    "\n",
    "def _kendall_tau_b(a: pd.Series, b: pd.Series) -> float:\n",
    "    a = pd.to_numeric(a, errors=\"coerce\")\n",
    "    b = pd.to_numeric(b, errors=\"coerce\")\n",
    "    m = a.notna() & b.notna()\n",
    "    if int(m.sum()) < 3:\n",
    "        return float(\"nan\")\n",
    "    try:\n",
    "        tau, _ = kendalltau(a[m], b[m], variant=\"b\")\n",
    "    except TypeError:\n",
    "        tau, _ = kendalltau(a[m], b[m])\n",
    "    return float(tau)\n",
    "\n",
    "\n",
    "def _pairwise_tau(df: pd.DataFrame, cols: list[str]) -> pd.Series:\n",
    "    taus = []\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            taus.append(_kendall_tau_b(df[cols[i]], df[cols[j]]))\n",
    "    return pd.Series(taus, dtype=float)\n",
    "\n",
    "\n",
    "def _cross_tau(df: pd.DataFrame, a_cols: list[str], b_cols: list[str]) -> pd.Series:\n",
    "    taus = []\n",
    "    for a in a_cols:\n",
    "        for b in b_cols:\n",
    "            taus.append(_kendall_tau_b(df[a], df[b]))\n",
    "    return pd.Series(taus, dtype=float)\n",
    "\n",
    "\n",
    "def _tau_summary(taus: pd.Series) -> dict:\n",
    "    taus = pd.to_numeric(taus, errors=\"coerce\").dropna()\n",
    "    if len(taus) == 0:\n",
    "        return {\"n_pairs\": 0, \"tau_median\": np.nan, \"tau_q25\": np.nan, \"tau_q75\": np.nan}\n",
    "    return {\n",
    "        \"n_pairs\": int(len(taus)),\n",
    "        \"tau_median\": float(taus.median()),\n",
    "        \"tau_q25\": float(taus.quantile(0.25)),\n",
    "        \"tau_q75\": float(taus.quantile(0.75)),\n",
    "    }\n",
    "\n",
    "\n",
    "tau_rows = []\n",
    "for metric in [\"dor\", \"ori\"]:\n",
    "    human_cols = [c for c in df_align.columns if c.startswith(f\"gr_{metric}_human_\")]\n",
    "    llm_cols = [\n",
    "        c\n",
    "        for c in df_align.columns\n",
    "        if c.startswith(f\"gr_{metric}_\")\n",
    "        and (\"human_\" not in c)\n",
    "        and (\"outlier\" not in c)\n",
    "        and (not c.endswith(\"_median\"))\n",
    "        and (not pd.api.types.is_bool_dtype(df_align[c]))\n",
    "    ]\n",
    "\n",
    "    s_h = _tau_summary(_pairwise_tau(df_align, human_cols))\n",
    "    s_l = _tau_summary(_pairwise_tau(df_align, llm_cols))\n",
    "    s_x = _tau_summary(_cross_tau(df_align, human_cols, llm_cols))\n",
    "\n",
    "    human_med = df_align[human_cols].apply(pd.to_numeric, errors=\"coerce\").median(axis=1, skipna=True)\n",
    "    llm_med = df_align[llm_cols].apply(pd.to_numeric, errors=\"coerce\").median(axis=1, skipna=True)\n",
    "    tau_med = _kendall_tau_b(human_med, llm_med)\n",
    "\n",
    "    tau_rows.append({\"metric\": metric.upper(), \"comparison\": \"humans_within\", **s_h})\n",
    "    tau_rows.append({\"metric\": metric.upper(), \"comparison\": \"llms_within\", **s_l})\n",
    "    tau_rows.append({\"metric\": metric.upper(), \"comparison\": \"humans_vs_llms_pairs\", **s_x})\n",
    "    tau_rows.append({\n",
    "        \"metric\": metric.upper(),\n",
    "        \"comparison\": \"ensemble_medians_human_vs_llm\",\n",
    "        \"n_pairs\": 1,\n",
    "        \"tau_median\": float(tau_med),\n",
    "        \"tau_q25\": np.nan,\n",
    "        \"tau_q75\": np.nan,\n",
    "    })\n",
    "\n",
    "\n",
    "tau_table = pd.DataFrame(tau_rows)\n",
    "for c in [\"tau_median\", \"tau_q25\", \"tau_q75\"]:\n",
    "    tau_table[c] = pd.to_numeric(tau_table[c], errors=\"coerce\").round(3)\n",
    "\n",
    "print(\"=\")\n",
    "print(\"Kendall τ_b summary\")\n",
    "print(tau_table.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e2b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "# Suppress noisy warnings (statsmodels/pandas)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "try:\n",
    "    from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Methodological diagnostics (same design: 135 items, 3 humans, 5 judges)\n",
    "# 1) Pairwise (human h, judge j) association distribution\n",
    "# 2) Ordinal model with rater severity (fixed-effects OrderedLogit)\n",
    "# 3) Per-rater standardization before aggregation\n",
    "\n",
    "if \"df_align\" not in globals():\n",
    "    raise NameError(\"Expected df_align from the previous chunk\")\n",
    "\n",
    "\n",
    "def _cols(prefix: str, df: pd.DataFrame) -> list[str]:\n",
    "    return [c for c in df.columns if c.startswith(prefix)]\n",
    "\n",
    "\n",
    "def _kendall_tau_b(a: pd.Series, b: pd.Series):\n",
    "    a = pd.to_numeric(a, errors=\"coerce\")\n",
    "    b = pd.to_numeric(b, errors=\"coerce\")\n",
    "    m = a.notna() & b.notna()\n",
    "    if int(m.sum()) < 3:\n",
    "        return float(\"nan\"), float(\"nan\"), int(m.sum())\n",
    "    try:\n",
    "        tau, p = kendalltau(a[m], b[m], variant=\"b\")\n",
    "    except TypeError:\n",
    "        tau, p = kendalltau(a[m], b[m])\n",
    "    return float(tau), float(p), int(m.sum())\n",
    "\n",
    "\n",
    "def _discretize_scores(s: pd.Series) -> pd.Series:\n",
    "    bins = [-np.inf, 1, 3, 5, 7, 9, 10]\n",
    "    labels = [\"[0,1]\", \"(1,3]\", \"(3,5]\", \"(5,7]\", \"(7,9]\", \"(9,10]\"]\n",
    "    return pd.cut(pd.to_numeric(s, errors=\"coerce\"), bins=bins, labels=labels, right=True, include_lowest=True)\n",
    "\n",
    "\n",
    "def _summary_iqr(x: pd.Series) -> dict:\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    return {\n",
    "        \"median\": float(x.median()),\n",
    "        \"q25\": float(x.quantile(0.25)),\n",
    "        \"q75\": float(x.quantile(0.75)),\n",
    "    }\n",
    "\n",
    "\n",
    "def pairwise_association(metric: str) -> pd.DataFrame:\n",
    "    human_cols = _cols(f\"gr_{metric}_human_\", df_align)\n",
    "    judge_cols = [\n",
    "        c\n",
    "        for c in _cols(f\"gr_{metric}_\", df_align)\n",
    "        if (\"human_\" not in c)\n",
    "        and (\"outlier\" not in c)\n",
    "        and (not c.endswith(\"_median\"))\n",
    "        and (not pd.api.types.is_bool_dtype(df_align[c]))\n",
    "    ]\n",
    "    if not human_cols or not judge_cols:\n",
    "        raise KeyError(f\"Missing columns for metric={metric}\")\n",
    "\n",
    "    rows = []\n",
    "    for h in human_cols:\n",
    "        for j in judge_cols:\n",
    "            tau, p, n = _kendall_tau_b(df_align[h], df_align[j])\n",
    "            rows.append({\n",
    "                \"metric\": metric.upper(),\n",
    "                \"human\": h,\n",
    "                \"judge\": j,\n",
    "                \"tau_b\": tau,\n",
    "                \"p\": p,\n",
    "                \"n\": n,\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def standardization_analysis(metric: str) -> dict:\n",
    "    human_cols = _cols(f\"gr_{metric}_human_\", df_align)\n",
    "    judge_cols = [\n",
    "        c\n",
    "        for c in _cols(f\"gr_{metric}_\", df_align)\n",
    "        if (\"human_\" not in c)\n",
    "        and (\"outlier\" not in c)\n",
    "        and (not c.endswith(\"_median\"))\n",
    "        and (not pd.api.types.is_bool_dtype(df_align[c]))\n",
    "    ]\n",
    "\n",
    "    h = df_align[human_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    j = df_align[judge_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    h_med = h.median(axis=1, skipna=True)\n",
    "    j_med = j.median(axis=1, skipna=True)\n",
    "    tau_raw, _, n_raw = _kendall_tau_b(h_med, j_med)\n",
    "\n",
    "    def zscore(col: pd.Series) -> pd.Series:\n",
    "        mu = col.mean(skipna=True)\n",
    "        sd = col.std(skipna=True, ddof=0)\n",
    "        if not np.isfinite(sd) or sd == 0:\n",
    "            return col * np.nan\n",
    "        return (col - mu) / sd\n",
    "\n",
    "    h_z = h.apply(zscore)\n",
    "    j_z = j.apply(zscore)\n",
    "    tau_z, _, n_z = _kendall_tau_b(h_z.median(axis=1, skipna=True), j_z.median(axis=1, skipna=True))\n",
    "\n",
    "    h_r = h.rank(axis=0, method=\"average\", na_option=\"keep\")\n",
    "    j_r = j.rank(axis=0, method=\"average\", na_option=\"keep\")\n",
    "    tau_rank, _, n_rank = _kendall_tau_b(h_r.median(axis=1, skipna=True), j_r.median(axis=1, skipna=True))\n",
    "\n",
    "    return {\n",
    "        \"metric\": metric.upper(),\n",
    "        \"tau_raw\": tau_raw,\n",
    "        \"n_raw\": n_raw,\n",
    "        \"tau_zscore\": tau_z,\n",
    "        \"n_zscore\": n_z,\n",
    "        \"tau_rank\": tau_rank,\n",
    "        \"n_rank\": n_rank,\n",
    "    }\n",
    "\n",
    "\n",
    "def ordinal_severity_model(metric: str):\n",
    "    \"\"\"Fixed-effects OrderedLogit: score_bin ~ item + rater.\n",
    "    This is not fully hierarchical shrinkage, but separates item signal vs rater severity.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] OrderedLogit needs statsmodels: {type(e).__name__}\")\n",
    "        return None\n",
    "\n",
    "    def to_long(cols: list[str], group: str) -> pd.DataFrame:\n",
    "        parts = []\n",
    "        for c in cols:\n",
    "            parts.append(pd.DataFrame({\n",
    "                \"item_id\": df_align[\"item_id\"] if \"item_id\" in df_align.columns else np.arange(1, len(df_align) + 1),\n",
    "                \"rater\": c,\n",
    "                \"group\": group,\n",
    "                \"score\": pd.to_numeric(df_align[c], errors=\"coerce\"),\n",
    "            }))\n",
    "        return pd.concat(parts, ignore_index=True)\n",
    "\n",
    "    human_cols = _cols(f\"gr_{metric}_human_\", df_align)\n",
    "    judge_cols = [\n",
    "        c\n",
    "        for c in _cols(f\"gr_{metric}_\", df_align)\n",
    "        if (\"human_\" not in c)\n",
    "        and (\"outlier\" not in c)\n",
    "        and (not c.endswith(\"_median\"))\n",
    "        and (not pd.api.types.is_bool_dtype(df_align[c]))\n",
    "    ]\n",
    "\n",
    "    long_h = to_long(human_cols, \"human\")\n",
    "    long_j = to_long(judge_cols, \"judge\")\n",
    "\n",
    "    def fit_group(long_df: pd.DataFrame):\n",
    "        y_cat = _discretize_scores(long_df[\"score\"]).cat.codes.replace(-1, np.nan)\n",
    "        m = y_cat.notna()\n",
    "        y = y_cat[m].astype(int)\n",
    "\n",
    "        X = pd.get_dummies(long_df.loc[m, [\"item_id\", \"rater\"]].astype({\"item_id\": str, \"rater\": str}), drop_first=True)\n",
    "        model = OrderedModel(y, X, distr=\"logit\")\n",
    "        res = model.fit(method=\"bfgs\", disp=False, maxiter=200)\n",
    "\n",
    "        # Extract item effects (baseline item = 0)\n",
    "        item_terms = [c for c in X.columns if c.startswith(\"item_id_\")]\n",
    "        rater_terms = [c for c in X.columns if c.startswith(\"rater_\")]\n",
    "\n",
    "        items = sorted(long_df[\"item_id\"].dropna().unique())\n",
    "        theta = {items[0]: 0.0}\n",
    "        for it in items[1:]:\n",
    "            key = f\"item_id_{it}\"\n",
    "            theta[it] = float(res.params.get(key, 0.0))\n",
    "\n",
    "        raters = sorted(long_df[\"rater\"].dropna().unique())\n",
    "        b = {raters[0]: 0.0}\n",
    "        for r in raters[1:]:\n",
    "            key = f\"rater_{r}\"\n",
    "            b[r] = float(res.params.get(key, 0.0))\n",
    "\n",
    "        return {\n",
    "            \"res\": res,\n",
    "            \"theta\": pd.Series(theta).sort_index(),\n",
    "            \"severity\": pd.Series(b),\n",
    "            \"n_obs\": int(m.sum()),\n",
    "            \"n_items\": int(len(items)),\n",
    "            \"n_raters\": int(len(raters)),\n",
    "        }\n",
    "\n",
    "    out_h = fit_group(long_h)\n",
    "    out_j = fit_group(long_j)\n",
    "\n",
    "    # Compare item latent effects across groups\n",
    "    common = out_h[\"theta\"].index.intersection(out_j[\"theta\"].index)\n",
    "    tau_theta, _, _ = _kendall_tau_b(out_h[\"theta\"].loc[common], out_j[\"theta\"].loc[common])\n",
    "\n",
    "    return {\n",
    "        \"metric\": metric.upper(),\n",
    "        \"human\": out_h,\n",
    "        \"judge\": out_j,\n",
    "        \"tau_theta_human_vs_judge\": tau_theta,\n",
    "        \"severity_human\": out_h[\"severity\"],\n",
    "        \"severity_judge\": out_j[\"severity\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def _human_cols(metric: str) -> list[str]:\n",
    "    return [c for c in df_align.columns if c.startswith(f\"gr_{metric}_human_\")]\n",
    "\n",
    "\n",
    "def _llm_cols(metric: str) -> list[str]:\n",
    "    return [\n",
    "        c\n",
    "        for c in _cols(f\"gr_{metric}_\", df_align)\n",
    "        if (\"human_\" not in c)\n",
    "        and (\"outlier\" not in c)\n",
    "        and (not c.endswith(\"_median\"))\n",
    "        and (not pd.api.types.is_bool_dtype(df_align[c]))\n",
    "    ]\n",
    "\n",
    "\n",
    "def _pairwise_within(cols: list[str], label: str, metric: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            tau, p, n = _kendall_tau_b(df_align[cols[i]], df_align[cols[j]])\n",
    "            rows.append({\n",
    "                \"metric\": metric.upper(),\n",
    "                \"group\": label,\n",
    "                \"rater_a\": cols[i],\n",
    "                \"rater_b\": cols[j],\n",
    "                \"tau_b\": tau,\n",
    "                \"n\": n,\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def _print_within(df_pairs: pd.DataFrame, title: str) -> None:\n",
    "    if df_pairs.empty:\n",
    "        print(f\"{title}: (no pairs)\")\n",
    "        return\n",
    "    s_all = _summary_iqr(df_pairs[\"tau_b\"])\n",
    "    print(title)\n",
    "    print(f\"pairs={len(df_pairs)} tau_b median={s_all['median']:.3f} IQR=[{s_all['q25']:.3f}, {s_all['q75']:.3f}]\")\n",
    "    show = df_pairs.sort_values(\"tau_b\")[[\"rater_a\", \"rater_b\", \"tau_b\", \"n\"]]\n",
    "    print(show.to_string(index=False))\n",
    "\n",
    "\n",
    "# Pre-fit ordinal models once (used in all sections)\n",
    "ord_dor = ordinal_severity_model(\"dor\")\n",
    "ord_ori = ordinal_severity_model(\"ori\")\n",
    "\n",
    "def _print_ordinal_group(obj: dict | None, group: str) -> None:\n",
    "    if obj is None:\n",
    "        print(\"[SKIP] OrderedLogit FE not available\")\n",
    "        return\n",
    "    out = obj[group]\n",
    "    sev = out[\"severity\"].sort_values()\n",
    "    print(f\"n_obs={out['n_obs']} n_items={out['n_items']} n_raters={out['n_raters']}\")\n",
    "    print(\"severity (lowest/highest):\")\n",
    "    print(pd.concat([sev.head(3), sev.tail(3)]).to_string())\n",
    "\n",
    "\n",
    "def _print_ordinal_ensemble(obj: dict | None) -> None:\n",
    "    if obj is None:\n",
    "        return\n",
    "    th = obj[\"human\"][\"theta\"]\n",
    "    tj = obj[\"judge\"][\"theta\"]\n",
    "    common = th.index.intersection(tj.index)\n",
    "    diff = (tj.loc[common] - th.loc[common]).astype(float)\n",
    "    print(f\"tau(theta_humans vs theta_llms) = {obj['tau_theta_human_vs_judge']:.3f}\")\n",
    "    print(f\"theta bias (llm - human): median={diff.median():.3f} IQR=[{diff.quantile(0.25):.3f}, {diff.quantile(0.75):.3f}]\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# HUMANS\n",
    "# -----------------------------\n",
    "print(\"=\")\n",
    "print(\"Methodological diagnostics — HUMANS\")\n",
    "for metric in [\"dor\", \"ori\"]:\n",
    "    cols = _human_cols(metric)\n",
    "    df_pairs = _pairwise_within(cols, label=\"humans\", metric=metric)\n",
    "    _print_within(df_pairs, f\"Within-humans τ_b ({metric.upper()})\")\n",
    "\n",
    "print(\"-\")\n",
    "print(\"Ordinal model (humans):\")\n",
    "_print_ordinal_group(ord_dor, \"human\")\n",
    "_print_ordinal_group(ord_ori, \"human\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LLMs\n",
    "# -----------------------------\n",
    "print(\"=\")\n",
    "print(\"Methodological diagnostics — LLMs\")\n",
    "for metric in [\"dor\", \"ori\"]:\n",
    "    cols = _llm_cols(metric)\n",
    "    df_pairs = _pairwise_within(cols, label=\"llms\", metric=metric)\n",
    "    _print_within(df_pairs, f\"Within-LLMs τ_b ({metric.upper()})\")\n",
    "\n",
    "print(\"-\")\n",
    "print(\"Ordinal model (LLMs):\")\n",
    "_print_ordinal_group(ord_dor, \"judge\")\n",
    "_print_ordinal_group(ord_ori, \"judge\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# ENSEMBLE MEDIANS (humans vs LLMs)\n",
    "# -----------------------------\n",
    "print(\"=\")\n",
    "print(\"Methodological diagnostics — Ensemble medians (humans vs LLMs)\")\n",
    "\n",
    "# 1) Cross-pairs (human × llm) distribution and judge ranking\n",
    "pair_dor = pairwise_association(\"dor\")\n",
    "pair_ori = pairwise_association(\"ori\")\n",
    "\n",
    "def _cross_print(df_pairs: pd.DataFrame, title: str):\n",
    "    s_all = _summary_iqr(df_pairs[\"tau_b\"])\n",
    "    by_j = df_pairs.groupby(\"judge\")[\"tau_b\"].median().sort_values()\n",
    "    print(title)\n",
    "    print(f\"pairs={len(df_pairs)} tau_b median={s_all['median']:.3f} IQR=[{s_all['q25']:.3f}, {s_all['q75']:.3f}]\")\n",
    "    print(\"lowest LLM judges (median over humans):\")\n",
    "    print(by_j.head(5).to_string())\n",
    "    print(\"highest LLM judges (median over humans):\")\n",
    "    print(by_j.tail(5).to_string())\n",
    "\n",
    "\n",
    "_cross_print(pair_dor, \"Cross human×LLM τ_b (DoR)\")\n",
    "_cross_print(pair_ori, \"Cross human×LLM τ_b (ORI)\")\n",
    "\n",
    "print(\"-\")\n",
    "print(\"Ordinal model (ensemble comparison):\")\n",
    "_print_ordinal_ensemble(ord_dor)\n",
    "_print_ordinal_ensemble(ord_ori)\n",
    "\n",
    "# 3) Per-rater standardization before aggregation\n",
    "std_dor = standardization_analysis(\"dor\")\n",
    "std_ori = standardization_analysis(\"ori\")\n",
    "print(\"-\")\n",
    "print(\"Standardization before aggregation (median humans vs median LLMs)\")\n",
    "for obj in [std_dor, std_ori]:\n",
    "    print(\n",
    "        f\"{obj['metric']}: tau_raw={obj['tau_raw']:.3f} (n={obj['n_raw']}) | \"\n",
    "        f\"tau_zscore={obj['tau_zscore']:.3f} (n={obj['n_zscore']}) | \"\n",
    "        f\"tau_rank={obj['tau_rank']:.3f} (n={obj['n_rank']})\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5f8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Where humans and LLM-judges agree/disagree the most\n",
    "\n",
    "if \"df_align\" not in globals():\n",
    "    # Try to construct df_align from df_human + model parquet (so this chunk can run standalone)\n",
    "    if \"df_human\" not in globals():\n",
    "        raise NameError(\n",
    "            \"Expected df_align (or df_human) from previous chunks. \"\n",
    "            \"Run the chunks that create df_human (responses import) and df_align (merge with model parquet).\"\n",
    "        )\n",
    "\n",
    "    MODEL_PARQUET = \"data/tidy_data_2_aed_model.parquet\"\n",
    "    try:\n",
    "        df_model = pd.read_parquet(MODEL_PARQUET)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            f\"Could not read {MODEL_PARQUET}. Ensure parquet support is installed (pyarrow or fastparquet). \"\n",
    "            f\"Original error: {type(e).__name__}: {e}\"\n",
    "        )\n",
    "\n",
    "    if \"_orig_index\" in df_human.columns:\n",
    "        if \"_orig_index\" not in df_model.columns:\n",
    "            df_model = df_model.reset_index().rename(columns={\"index\": \"_orig_index\"})\n",
    "        df_align = df_human.merge(df_model, on=\"_orig_index\", how=\"left\", suffixes=(\"\", \"_model\"))\n",
    "    elif \"item_id\" in df_model.columns and \"item_id\" in df_human.columns:\n",
    "        df_align = df_human.merge(df_model, on=\"item_id\", how=\"left\", suffixes=(\"\", \"_model\"))\n",
    "    else:\n",
    "        raise KeyError(\n",
    "            \"Could not join df_human to df_model. Need either '_orig_index' in df_human, or 'item_id' in both.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def _discretize_rubric(s: pd.Series) -> pd.Series:\n",
    "    bins = [-np.inf, 1, 3, 5, 7, 9, 10]\n",
    "    labels = [\"[0,1]\", \"(1,3]\", \"(3,5]\", \"(5,7]\", \"(7,9]\", \"(9,10]\"]\n",
    "    return pd.cut(pd.to_numeric(s, errors=\"coerce\"), bins=bins, labels=labels, right=True, include_lowest=True)\n",
    "\n",
    "\n",
    "def _human_cols(metric: str) -> list[str]:\n",
    "    return [c for c in df_align.columns if c.startswith(f\"gr_{metric}_human_\")]\n",
    "\n",
    "\n",
    "def _llm_cols(metric: str) -> list[str]:\n",
    "    return [\n",
    "        c\n",
    "        for c in df_align.columns\n",
    "        if c.startswith(f\"gr_{metric}_\")\n",
    "        and (\"human_\" not in c)\n",
    "        and (\"outlier\" not in c)\n",
    "        and (not c.endswith(\"_median\"))\n",
    "        and (not pd.api.types.is_bool_dtype(df_align[c]))\n",
    "    ]\n",
    "\n",
    "\n",
    "META_COLS = [c for c in [\"item_id\", \"_orig_index\", \"source\", \"model\", \"prompt_type\", \"item\", \"answer\"] if c in df_align.columns]\n",
    "\n",
    "\n",
    "def _item_agreement(metric: str) -> pd.DataFrame:\n",
    "    h_cols = _human_cols(metric)\n",
    "    l_cols = _llm_cols(metric)\n",
    "    if not h_cols or not l_cols:\n",
    "        raise KeyError(f\"Missing columns for metric={metric}\")\n",
    "\n",
    "    h = df_align[h_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    l = df_align[l_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    h_med = h.median(axis=1, skipna=True)\n",
    "    l_med = l.median(axis=1, skipna=True)\n",
    "\n",
    "    out = df_align[META_COLS].copy() if META_COLS else pd.DataFrame(index=df_align.index)\n",
    "    out[f\"{metric}_human_median\"] = h_med\n",
    "    out[f\"{metric}_llm_median\"] = l_med\n",
    "    out[f\"{metric}_abs_diff\"] = (h_med - l_med).abs()\n",
    "\n",
    "    out[f\"{metric}_human_iqr\"] = h.quantile(0.75, axis=1) - h.quantile(0.25, axis=1)\n",
    "    out[f\"{metric}_llm_iqr\"] = l.quantile(0.75, axis=1) - l.quantile(0.25, axis=1)\n",
    "\n",
    "    hb = _discretize_rubric(h_med)\n",
    "    lb = _discretize_rubric(l_med)\n",
    "    out[f\"{metric}_human_bin\"] = hb.astype(str)\n",
    "    out[f\"{metric}_llm_bin\"] = lb.astype(str)\n",
    "    out[f\"{metric}_bin_match\"] = (hb == lb)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "dor_items = _item_agreement(\"dor\")\n",
    "ori_items = _item_agreement(\"ori\")\n",
    "\n",
    "# Combine for an overall view\n",
    "combo = dor_items.merge(\n",
    "    ori_items[[c for c in ori_items.columns if c not in META_COLS]],\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how=\"inner\",\n",
    ")\n",
    "combo[\"abs_diff_mean_dor_ori\"] = combo[[\"dor_abs_diff\", \"ori_abs_diff\"]].mean(axis=1)\n",
    "combo[\"both_bins_match\"] = combo[\"dor_bin_match\"] & combo[\"ori_bin_match\"]\n",
    "\n",
    "def _print_top(df: pd.DataFrame, title: str, sort_col: str, ascending: bool, n: int = 10):\n",
    "    cols = META_COLS + [\n",
    "        \"dor_human_median\",\n",
    "        \"dor_llm_median\",\n",
    "        \"dor_abs_diff\",\n",
    "        \"dor_human_bin\",\n",
    "        \"dor_llm_bin\",\n",
    "        \"dor_bin_match\",\n",
    "        \"ori_human_median\",\n",
    "        \"ori_llm_median\",\n",
    "        \"ori_abs_diff\",\n",
    "        \"ori_human_bin\",\n",
    "        \"ori_llm_bin\",\n",
    "        \"ori_bin_match\",\n",
    "        \"abs_diff_mean_dor_ori\",\n",
    "        \"both_bins_match\",\n",
    "    ]\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    view = df.sort_values(sort_col, ascending=ascending).head(n)[cols].copy()\n",
    "    for c in [\"dor_human_median\",\"dor_llm_median\",\"dor_abs_diff\",\"ori_human_median\",\"ori_llm_median\",\"ori_abs_diff\",\"abs_diff_mean_dor_ori\"]:\n",
    "        if c in view.columns:\n",
    "            view[c] = pd.to_numeric(view[c], errors=\"coerce\").round(3)\n",
    "    print(\"=\")\n",
    "    print(title)\n",
    "    print(view.to_string(index=False))\n",
    "\n",
    "\n",
    "_print_top(combo, \"Most agreement (lowest mean |Δ| across DoR/ORI)\", \"abs_diff_mean_dor_ori\", ascending=True)\n",
    "_print_top(combo, \"Most disagreement (highest mean |Δ| across DoR/ORI)\", \"abs_diff_mean_dor_ori\", ascending=False)\n",
    "\n",
    "# Trait-specific views\n",
    "_print_top(combo, \"Most agreement (DoR)\", \"dor_abs_diff\", ascending=True)\n",
    "_print_top(combo, \"Most disagreement (DoR)\", \"dor_abs_diff\", ascending=False)\n",
    "_print_top(combo, \"Most agreement (ORI)\", \"ori_abs_diff\", ascending=True)\n",
    "_print_top(combo, \"Most disagreement (ORI)\", \"ori_abs_diff\", ascending=False)\n",
    "\n",
    "# Quick counts\n",
    "print(\"=\")\n",
    "print(\"Bin agreement rates (rubric bins)\")\n",
    "print(pd.DataFrame({\n",
    "    \"metric\": [\"DoR\", \"ORI\", \"both\"],\n",
    "    \"bin_match_rate\": [\n",
    "        float(combo[\"dor_bin_match\"].mean()),\n",
    "        float(combo[\"ori_bin_match\"].mean()),\n",
    "        float(combo[\"both_bins_match\"].mean()),\n",
    "    ],\n",
    "}).assign(bin_match_rate=lambda d: d[\"bin_match_rate\"].round(3)).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
