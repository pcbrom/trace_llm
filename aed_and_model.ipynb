{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_parquet('data/tidy_data_2_aed_model.parquet')\n",
    "gr_dor_columns = df.columns[df.columns.str.startswith('gr_dor')]\n",
    "df['gr_dor_median'] = df[gr_dor_columns].median(axis=1)\n",
    "gr_ori_columns = df.columns[df.columns.str.startswith('gr_ori')]\n",
    "df['gr_ori_median'] = df[gr_ori_columns].median(axis=1)\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === Configuration ===\n",
    "model_name_map = {\n",
    "    \"gpt_4o_mini\": \"4o-mini\",\n",
    "    \"gpt_41_nano\": \"4.1-nano\",\n",
    "    \"claude_35_haiku\": \"3-haiku\",\n",
    "    \"grok_3_mini_beta\": \"grok-3-mini\",\n",
    "    \"ds_v3\": \"ds-v3\"\n",
    "}\n",
    "\n",
    "# === gr_dor processing ===\n",
    "df_box_dor = df[gr_dor_columns.tolist() + ['gr_dor_median']].copy()\n",
    "df_box_dor.columns = [col.replace('gr_dor_', '') for col in df_box_dor.columns]\n",
    "df_box_dor.rename(columns={k: model_name_map.get(k, k) for k in df_box_dor.columns}, inplace=True)\n",
    "df_long_dor = df_box_dor.melt(var_name='Model', value_name='Score')\n",
    "df_long_dor['Criterion'] = 'Deep of Reasoning'\n",
    "\n",
    "# === gr_ori processing ===\n",
    "df_box_ori = df[gr_ori_columns.tolist() + ['gr_ori_median']].copy()\n",
    "df_box_ori.columns = [col.replace('gr_ori_', '') for col in df_box_ori.columns]\n",
    "df_box_ori.rename(columns={k: model_name_map.get(k, k) for k in df_box_ori.columns}, inplace=True)\n",
    "df_long_ori = df_box_ori.melt(var_name='Model', value_name='Score')\n",
    "df_long_ori['Criterion'] = 'Originality Score'\n",
    "\n",
    "# === Combine into one DataFrame ===\n",
    "df_long_combined = pd.concat([df_long_dor, df_long_ori], ignore_index=True)\n",
    "\n",
    "# === Single grouped boxplot ===\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "boxplot = sns.boxplot(\n",
    "    data=df_long_combined,\n",
    "    x='Model',\n",
    "    y='Score',\n",
    "    hue='Criterion',\n",
    "    palette='rocket_r',\n",
    "    linewidth=1.5,\n",
    "    fliersize=5,\n",
    "    dodge=True,\n",
    "    flierprops=dict(marker='o', markerfacecolor='red', markersize=4)  # Change outlier shape\n",
    ")\n",
    "\n",
    "# Enhance the aesthetics\n",
    "plt.title('Comparison of Deep of Reasoning and Originality scores by model', fontsize=16)\n",
    "plt.xlabel('', fontsize=14)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.xticks(rotation=30, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(title='Criterion', loc='lower left', fontsize=10)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/comparison_deep_of_reasoning_and_originality_scores.png', dpi=360)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# === Summary statistics ===\n",
    "def compute_summary(columns, prefix, alias_map, metric_label):\n",
    "    rows = []\n",
    "    for col in columns:\n",
    "        raw_name = col.replace(prefix, '')\n",
    "        name = alias_map.get(raw_name, raw_name)\n",
    "        data = df[col]\n",
    "        rows.append({\n",
    "            'Criterion': metric_label,\n",
    "            'Model': name,\n",
    "            'Mean': data.mean(),\n",
    "            'Std': data.std(),\n",
    "            'Median': data.median(),\n",
    "            'Min': data.min(),\n",
    "            'Max': data.max(),\n",
    "            'Coefficient of Variation': data.std() / data.mean() if data.mean() != 0 else None\n",
    "        })\n",
    "    # Add median row\n",
    "    median_col = f'{prefix}median'\n",
    "    rows.append({\n",
    "        'Criterion': metric_label,\n",
    "        'Model': 'median',\n",
    "        'Mean': df[median_col].mean(),\n",
    "        'Std': df[median_col].std(),\n",
    "        'Median': df[median_col].median(),\n",
    "        'Min': df[median_col].min(),\n",
    "        'Max': df[median_col].max(),\n",
    "        'Coefficient of Variation': df[median_col].std() / df[median_col].mean()\n",
    "    })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "summary_dor = compute_summary(gr_dor_columns, 'gr_dor_', model_name_map, 'Deep of Reasoning')\n",
    "summary_ori = compute_summary(gr_ori_columns, 'gr_ori_', model_name_map, 'Originality Score')\n",
    "\n",
    "summary_combined = pd.concat([summary_dor, summary_ori], ignore_index=True)\n",
    "summary_combined = summary_combined.round(4).sort_values(by=['Criterion', 'Model']).reset_index(drop=True)\n",
    "\n",
    "# Display summary table\n",
    "display(summary_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Alias mapping\n",
    "model_name_map = {\n",
    "    \"gpt_4o_mini\": \"4o-mini\",\n",
    "    \"gpt_41_nano\": \"4.1-nano\",\n",
    "    \"claude_35_haiku\": \"3-haiku\",\n",
    "    \"grok_3_mini_beta\": \"grok-3-mini\",\n",
    "    \"ds_v3\": \"ds-v3\"\n",
    "}\n",
    "\n",
    "# Prepare correlation matrix\n",
    "df2cor = pd.concat([df[gr_dor_columns], df['gr_dor_median'], df[gr_ori_columns], df['gr_ori_median']], axis=1)\n",
    "correlation_matrix = df2cor.corr(method='kendall')\n",
    "\n",
    "# Apply alias names and remove prefixes\n",
    "def clean_label(label):\n",
    "    for key in model_name_map:\n",
    "        if key in label:\n",
    "            label = label.replace(key, model_name_map[key])\n",
    "    label = label.replace('gr_dor_', '').replace('gr_ori_', '').replace('_median', ' median')\n",
    "    return label\n",
    "\n",
    "renamed_columns = [clean_label(col) for col in df2cor.columns]\n",
    "correlation_matrix.columns = renamed_columns\n",
    "correlation_matrix.index = renamed_columns\n",
    "\n",
    "# Mask the lower triangle\n",
    "mask = np.tril(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='rocket_r', square=True,\n",
    "            cbar_kws={\"shrink\": .8}, linewidths=0.5, mask=mask, annot_kws={\"size\": 10})\n",
    "\n",
    "# Calculate separation index\n",
    "dor_sep_index = len(gr_dor_columns) + 1\n",
    "\n",
    "# Add horizontal and vertical lines at the separation index\n",
    "plt.axhline(y=dor_sep_index, color='black', linewidth=1.5)\n",
    "plt.axvline(x=dor_sep_index, color='black', linewidth=1.5)\n",
    "\n",
    "# Add quadrant labels outside the heatmap (below and left)\n",
    "n = correlation_matrix.shape[0]\n",
    "plt.text(dor_sep_index / 2, n - 12.2, r'$\\mathbf{DoR \\ x \\ DoR}$', fontsize=10, ha='center', color='blue')\n",
    "plt.text((dor_sep_index + n) / 2, n - 12.2, r'$\\mathbf{DoR \\ x \\ ORI}$', fontsize=10, ha='center', color='green')\n",
    "plt.text(n + 0.2, dor_sep_index / 2, r'$\\mathbf{ORI \\ x \\ DoR}$', fontsize=10, va='center', rotation=270, color='green')\n",
    "plt.text(n + 0.2, (dor_sep_index + n) / 2, r'$\\mathbf{ORI \\ x \\ ORI}$', fontsize=10, va='center', rotation=270, color='firebrick')\n",
    "\n",
    "# Add axis group labels\n",
    "plt.text(dor_sep_index / 2, 12, 'Group DoR', fontsize=12, ha='center', color='black')\n",
    "plt.text((dor_sep_index + n) / 2, 12, 'Group ORI', fontsize=12, ha='center', color='black')\n",
    "plt.text(0, dor_sep_index / 2, 'Group DoR', fontsize=12, va='center', rotation=90, color='black')\n",
    "plt.text(0, (dor_sep_index + n) / 2, 'Group ORI', fontsize=12, va='center', rotation=90, color='black')\n",
    "\n",
    "# Adjust aesthetics\n",
    "plt.title('Tau Kendall Correlation Matrix of Deep of Reasoning (DoR)\\nand Originality (ORI) Scores', fontsize=16, pad=30)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/kendall_correlogram.png', dpi=360)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Alias mapping\n",
    "model_name_map = {\n",
    "    \"gpt_4o_mini\": \"4o-mini\",\n",
    "    \"gpt_41_nano\": \"4.1-nano\",\n",
    "    \"claude_35_haiku\": \"3-haiku\",\n",
    "    \"grok_3_mini_beta\": \"grok-3-mini\",\n",
    "    \"ds_v3\": \"ds-v3\"\n",
    "}\n",
    "\n",
    "def clean_label(label):\n",
    "    for key in model_name_map:\n",
    "        if key in label:\n",
    "            label = label.replace(key, model_name_map[key])\n",
    "    label = label.replace('gr_dor_', '').replace('gr_ori_', '').replace('_median', ' median')\n",
    "    return label\n",
    "\n",
    "def bootstrap_correlation(df_subset, n_iterations=100):\n",
    "    \"\"\"Perform bootstrap to calculate confidence intervals for Kendall's tau.\"\"\"\n",
    "    tau_values = []\n",
    "    for _ in range(n_iterations):\n",
    "        sample = df_subset.sample(frac=1, replace=True)\n",
    "        tau = sample.corr(method='kendall').iloc[0, 1]  # Assuming we want tau between first two columns\n",
    "        tau_values.append(tau)\n",
    "    lower_bound = np.percentile(tau_values, 2.5)\n",
    "    upper_bound = np.percentile(tau_values, 97.5)\n",
    "    return lower_bound, upper_bound, tau_values\n",
    "\n",
    "def hypothesis_test(tau_values):\n",
    "    \"\"\"Perform hypothesis test for Kendall's tau.\"\"\"\n",
    "    p_value = (np.sum(np.array(tau_values) <= 0) + np.sum(np.array(tau_values) >= 0)) / len(tau_values)  # Two-tailed test\n",
    "    return p_value\n",
    "\n",
    "def plot_correlogram(df_subset, title, filename=None, ax=None, annotate=True):\n",
    "    df2cor = pd.concat([\n",
    "        df_subset[gr_dor_columns], df_subset['gr_dor_median'],\n",
    "        df_subset[gr_ori_columns], df_subset['gr_ori_median']\n",
    "    ], axis=1)\n",
    "    correlation_matrix = df2cor.corr(method='kendall')\n",
    "\n",
    "    renamed_columns = [clean_label(col) for col in df2cor.columns]\n",
    "    correlation_matrix.columns = renamed_columns\n",
    "    correlation_matrix.index = renamed_columns\n",
    "\n",
    "    # Display the correlation matrix\n",
    "    print(f\"Displaying the Kendall correlation matrix for: {title}.\")\n",
    "    display(correlation_matrix)\n",
    "\n",
    "    # Calculate and display bootstrap confidence intervals and hypothesis test results\n",
    "    lower_bound, upper_bound, tau_values = bootstrap_correlation(df2cor)\n",
    "    p_value = hypothesis_test(tau_values)\n",
    "    print(f\"95% Confidence Interval for τ: [{lower_bound:.3f}, {upper_bound:.3f}]\")\n",
    "    print(f\"P-value for hypothesis test: {p_value:.3f}\")\n",
    "\n",
    "    mask = np.tril(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "    sns.heatmap(correlation_matrix, annot=annotate, fmt=\".2f\", cmap='rocket_r', square=True,\n",
    "                cbar_kws={\"shrink\": .8} if annotate else None, linewidths=0.5, mask=mask,\n",
    "                annot_kws={\"size\": 10} if annotate else {}, ax=ax)\n",
    "\n",
    "    dor_sep_index = len(gr_dor_columns) + 1\n",
    "    n = correlation_matrix.shape[0]\n",
    "\n",
    "    ax.axhline(y=dor_sep_index, color='black', linewidth=1.5)\n",
    "    ax.axvline(x=dor_sep_index, color='black', linewidth=1.5)\n",
    "\n",
    "    ax.text(dor_sep_index / 2, n - 12.2, r'$\\mathbf{DoR \\ x \\ DoR}$', fontsize=10, ha='center', color='blue')\n",
    "    ax.text((dor_sep_index + n) / 2, n - 12.2, r'$\\mathbf{DoR \\ x \\ ORI}$', fontsize=10, ha='center', color='green')\n",
    "    ax.text(n + 0.2, dor_sep_index / 2, r'$\\mathbf{ORI \\ x \\ DoR}$', fontsize=10, va='center', rotation=270, color='green')\n",
    "    ax.text(n + 0.2, (dor_sep_index + n) / 2, r'$\\mathbf{ORI \\ x \\ ORI}$', fontsize=10, va='center', rotation=270, color='firebrick')\n",
    "\n",
    "    ax.text(dor_sep_index / 2, 12, 'Group DoR', fontsize=12, ha='center', color='black')\n",
    "    ax.text((dor_sep_index + n) / 2, 12, 'Group ORI', fontsize=12, ha='center', color='black')\n",
    "    ax.text(0, dor_sep_index / 2, 'Group DoR', fontsize=12, va='center', rotation=90, color='black')\n",
    "    ax.text(0, (dor_sep_index + n) / 2, 'Group ORI', fontsize=12, va='center', rotation=90, color='black')\n",
    "\n",
    "    ax.set_title(title, fontsize=14, pad=12)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.tick_params(axis='y', rotation=0)\n",
    "\n",
    "    if filename and ax is None:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename, dpi=360)\n",
    "        plt.close()\n",
    "\n",
    "# Create combined 2x2 plot with full + prompt_type\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 13))\n",
    "fig.suptitle('Tau Kendall Correlation Matrix of Deep of Reasoning (DoR)\\nand Originality (ORI) Scores by Prompt Type', fontsize=16)\n",
    "\n",
    "# Plot for all data\n",
    "plot_correlogram(df, 'All Data', ax=axes[0, 0], annotate=True)\n",
    "\n",
    "# Display correlations for all prompt types\n",
    "prompt_types = df['prompt_type'].dropna().unique()\n",
    "for ax, prompt in zip(axes.flat[1:], prompt_types):\n",
    "    df_subset = df[df['prompt_type'] == prompt]\n",
    "    plot_correlogram(df_subset, prompt, ax=ax, annotate=True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "plt.savefig('figures/kendall_correlogram_combined.png', dpi=360)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Alias mapping\n",
    "model_name_map = {\n",
    "    \"gpt_4o_mini\": \"4o-mini\",\n",
    "    \"gpt_41_nano\": \"4.1-nano\",\n",
    "    \"claude_35_haiku\": \"3-haiku\",\n",
    "    \"grok_3_mini_beta\": \"grok-3-mini\",\n",
    "    \"ds_v3\": \"ds-v3\"\n",
    "}\n",
    "\n",
    "def clean_label(label):\n",
    "    for key in model_name_map:\n",
    "        if key in label:\n",
    "            label = label.replace(key, model_name_map[key])\n",
    "    label = label.replace('gr_dor_', '').replace('gr_ori_', '').replace('_median', ' median')\n",
    "    return label\n",
    "\n",
    "def plot_correlogram(df_subset, title, filename=None, ax=None, annotate=True):\n",
    "    df2cor = pd.concat([\n",
    "        df_subset[gr_dor_columns], df_subset['gr_dor_median'],\n",
    "        df_subset[gr_ori_columns], df_subset['gr_ori_median']\n",
    "    ], axis=1)\n",
    "    correlation_matrix = df2cor.corr(method='kendall')\n",
    "\n",
    "    renamed_columns = [clean_label(col) for col in df2cor.columns]\n",
    "    correlation_matrix.columns = renamed_columns\n",
    "    correlation_matrix.index = renamed_columns\n",
    "\n",
    "    # Display the correlation matrix\n",
    "    print(f\"Displaying the Kendall correlation matrix for: {title}.\")\n",
    "    display(correlation_matrix)\n",
    "\n",
    "    mask = np.tril(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "    sns.heatmap(correlation_matrix, annot=annotate, fmt=\".2f\", cmap='rocket_r', square=True,\n",
    "                cbar_kws={\"shrink\": .8} if annotate else None, linewidths=0.5, mask=mask,\n",
    "                annot_kws={\"size\": 10} if annotate else {}, ax=ax)\n",
    "\n",
    "    dor_sep_index = len(gr_dor_columns) + 1\n",
    "    n = correlation_matrix.shape[0]\n",
    "\n",
    "    ax.axhline(y=dor_sep_index, color='black', linewidth=1.5)\n",
    "    ax.axvline(x=dor_sep_index, color='black', linewidth=1.5)\n",
    "\n",
    "    ax.text(dor_sep_index / 2, n - 12.2, r'$\\mathbf{DoR \\ x \\ DoR}$', fontsize=10, ha='center', color='blue')\n",
    "    ax.text((dor_sep_index + n) / 2, n - 12.2, r'$\\mathbf{DoR \\ x \\ ORI}$', fontsize=10, ha='center', color='green')\n",
    "    ax.text(n + 0.2, dor_sep_index / 2, r'$\\mathbf{ORI \\ x \\ DoR}$', fontsize=10, va='center', rotation=270, color='green')\n",
    "    ax.text(n + 0.2, (dor_sep_index + n) / 2, r'$\\mathbf{ORI \\ x \\ ORI}$', fontsize=10, va='center', rotation=270, color='firebrick')\n",
    "\n",
    "    ax.text(dor_sep_index / 2, 12, 'Group DoR', fontsize=12, ha='center', color='black')\n",
    "    ax.text((dor_sep_index + n) / 2, 12, 'Group ORI', fontsize=12, ha='center', color='black')\n",
    "    ax.text(0, dor_sep_index / 2, 'Group DoR', fontsize=12, va='center', rotation=90, color='black')\n",
    "    ax.text(0, (dor_sep_index + n) / 2, 'Group ORI', fontsize=12, va='center', rotation=90, color='black')\n",
    "\n",
    "    ax.set_title(title, fontsize=16, pad=20)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.tick_params(axis='y', rotation=0)\n",
    "\n",
    "    if filename and ax is None:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename, dpi=360)\n",
    "        plt.close()\n",
    "\n",
    "# Create combined 3x3 plot with prompt_type and source\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 20))\n",
    "fig.suptitle('Tau Kendall Correlation Matrix of Deep of Reasoning (DoR) and\\nOriginality (ORI) Scores by Prompt Type and Source', fontsize=20)\n",
    "\n",
    "# Display correlations for selected prompt types and sources\n",
    "prompt_types = df['prompt_type'].dropna().unique()  # Select first three prompt types\n",
    "sources = df['source'].dropna().unique()  # Select first three sources\n",
    "for ax, (prompt, source) in zip(axes.flat, [(p, s) for p in prompt_types for s in sources]):\n",
    "    df_subset = df[(df['prompt_type'] == prompt) & (df['source'] == source)]\n",
    "    plot_correlogram(df_subset, f'{prompt} - {source}', ax=ax, annotate=True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig('figures/kendall_correlogram_combined_source_prompt.png', dpi=360)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1: Reasoning depth versus alternative correctness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === Configuration ===\n",
    "model_name_map = {\n",
    "    \"gpt_4o_mini\": \"4o-mini\",\n",
    "    \"gpt_41_nano\": \"4.1-nano\",\n",
    "    \"claude_35_haiku\": \"3-haiku\",\n",
    "    \"grok_3_mini_beta\": \"grok-3-mini\",\n",
    "    \"ds_v3\": \"ds-v3\"\n",
    "}\n",
    "\n",
    "# === Process gr_dor ===\n",
    "df_dor = df[gr_dor_columns.tolist() + ['gr_dor_median', 'hit_alternative']].copy()\n",
    "df_dor.columns = [col.replace('gr_dor_', '') for col in df_dor.columns]\n",
    "df_dor.rename(columns={k: model_name_map.get(k, k) for k in df_dor.columns}, inplace=True)\n",
    "df_long_dor = df_dor.melt(id_vars='hit_alternative', var_name='Model', value_name='Score')\n",
    "df_long_dor['Criterion'] = 'Deep of Reasoning'\n",
    "\n",
    "# === Process gr_ori ===\n",
    "df_ori = df[gr_ori_columns.tolist() + ['gr_ori_median', 'hit_alternative']].copy()\n",
    "df_ori.columns = [col.replace('gr_ori_', '') for col in df_ori.columns]\n",
    "df_ori.rename(columns={k: model_name_map.get(k, k) for k in df_ori.columns}, inplace=True)\n",
    "df_long_ori = df_ori.melt(id_vars='hit_alternative', var_name='Model', value_name='Score')\n",
    "df_long_ori['Criterion'] = 'Originality Score'\n",
    "\n",
    "# === Combine and relabel hit_alternative ===\n",
    "df_long_combined = pd.concat([df_long_dor, df_long_ori], ignore_index=True)\n",
    "df_long_combined['hit_alternative'] = df_long_combined['hit_alternative'].map({0: 'Fail', 1: 'Success'})\n",
    "\n",
    "# === Plot with FacetGrid (one figure per Criterion) ===\n",
    "sns.set(style=\"whitegrid\")\n",
    "g = sns.FacetGrid(\n",
    "    df_long_combined,\n",
    "    col='Criterion',\n",
    "    height=6,  # Updated figure height\n",
    "    aspect=1.0,  # Updated aspect ratio to maintain overall size\n",
    "    sharey=False\n",
    ")\n",
    "g.map_dataframe(\n",
    "    sns.boxplot,\n",
    "    x='Model',\n",
    "    y='Score',\n",
    "    hue='hit_alternative',\n",
    "    palette='rocket_r',\n",
    "    linewidth=1.5,\n",
    "    fliersize=4,\n",
    "    dodge=True,\n",
    "    flierprops=dict(marker='o', markerfacecolor='red', markersize=4)\n",
    ")\n",
    "\n",
    "# === Formatting ===\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "g.set_axis_labels(\"\", \"Score\")\n",
    "for ax in g.axes.flat:\n",
    "    ax.tick_params(axis='x', rotation=30)\n",
    "g.add_legend(title='Alternative Hit', loc='lower center', bbox_to_anchor=(0.5, -0.1), ncol=2)\n",
    "plt.subplots_adjust(top=0.85)\n",
    "g.fig.suptitle('Evaluation Scores by Model and Alternative Hit Status', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/faceted_scores_by_hit_status.png', dpi=360, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# === Compute summary by Model, Criterion, and Alternative Hit ===\n",
    "summary_rows = []\n",
    "\n",
    "for criterion, prefix, cols in [\n",
    "    (\"Deep of Reasoning\", \"gr_dor_\", gr_dor_columns),\n",
    "    (\"Originality Score\", \"gr_ori_\", gr_ori_columns)\n",
    "]:\n",
    "    for col in cols:\n",
    "        raw_name = col.replace(prefix, '')\n",
    "        model = model_name_map.get(raw_name, raw_name)\n",
    "\n",
    "        for hit_value, hit_label in zip([0, 1], ['Fail', 'Success']):\n",
    "            subset = df[df['hit_alternative'] == hit_value][col]\n",
    "            summary_rows.append({\n",
    "                'Criterion': criterion,\n",
    "                'Model': model,\n",
    "                'Alternative Hit': hit_label,\n",
    "                'Mean': subset.mean(),\n",
    "                'Std': subset.std(),\n",
    "                'Median': subset.median(),\n",
    "                'Min': subset.min(),\n",
    "                'Max': subset.max(),\n",
    "                'Coefficient of Variation': subset.std() / subset.mean() if subset.mean() != 0 else None\n",
    "            })\n",
    "\n",
    "    # Add median column\n",
    "    median_col = f'{prefix}median'\n",
    "    for hit_value, hit_label in zip([0, 1], ['Fail', 'Success']):\n",
    "        subset = df[df['hit_alternative'] == hit_value][median_col]\n",
    "        summary_rows.append({\n",
    "            'Criterion': criterion,\n",
    "            'Model': 'median',\n",
    "            'Alternative Hit': hit_label,\n",
    "            'Mean': subset.mean(),\n",
    "            'Std': subset.std(),\n",
    "            'Median': subset.median(),\n",
    "            'Min': subset.min(),\n",
    "            'Max': subset.max(),\n",
    "            'Coefficient of Variation': subset.std() / subset.mean()\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame and sort\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df = summary_df.round(4).sort_values(by=['Criterion', 'Alternative Hit', 'Model']).reset_index(drop=True)\n",
    "\n",
    "# Display or export\n",
    "display(summary_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Adversarial Compensation Effect (ACE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Model aliases\n",
    "model_name_map = {\n",
    "    \"gpt_4o_mini\": \"4o-mini\",\n",
    "    \"gpt_41_nano\": \"4.1-nano\",\n",
    "    \"claude_35_haiku\": \"3-haiku\",\n",
    "    \"grok_3_mini_beta\": \"grok-3-mini\",\n",
    "    \"ds_v3\": \"ds-v3\"\n",
    "}\n",
    "\n",
    "# Prompt types\n",
    "model_list = list(model_name_map.keys())\n",
    "prompt_types = ['naive', 'cot', 'adversarial']\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for model_substring in model_list:\n",
    "    model_match = next((m for m in df['model'].unique() if model_substring in m), None)\n",
    "    if model_match is None:\n",
    "        print(f\"[!] Model '{model_substring}' not found.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        dor_col = next(c for c in df.columns if f\"gr_dor_{model_substring}\" in c)\n",
    "        ori_col = next(c for c in df.columns if f\"gr_ori_{model_substring}\" in c)\n",
    "    except StopIteration:\n",
    "        print(f\"[!] Trait columns not found for '{model_substring}'\")\n",
    "        continue\n",
    "\n",
    "    df_model = df[df[\"model\"] == model_match]\n",
    "\n",
    "    wf1_means = []\n",
    "    dor_means = []\n",
    "    dor_vars = []\n",
    "    ori_means = []\n",
    "    ori_vars = []\n",
    "    kendall_dor_taus = []\n",
    "    kendall_ori_taus = []\n",
    "\n",
    "    for prompt in prompt_types:\n",
    "        subset = df_model[df_model[\"prompt_type\"] == prompt]\n",
    "\n",
    "        # WF₁\n",
    "        if subset.empty:\n",
    "            wf1 = 0.0\n",
    "        else:\n",
    "            y_true = subset[\"answer\"]\n",
    "            y_pred = subset[\"model_answer\"]\n",
    "\n",
    "            # remove NaNs e garante strings limpas (A/B/C/D/E)\n",
    "            mask = y_true.notna() & y_pred.notna()\n",
    "            y_true = y_true[mask].astype(str).str.strip().str.upper()\n",
    "            y_pred = y_pred[mask].astype(str).str.strip().str.upper()\n",
    "\n",
    "            if len(y_true) == 0:\n",
    "                wf1 = 0.0\n",
    "            else:\n",
    "                # IMPORTANTe: incluir também labels que apareçam só no y_pred (ex: 'E')\n",
    "                labels = sorted(set(y_true.unique()) | set(y_pred.unique()))\n",
    "                wf1 = f1_score(\n",
    "                    y_true, y_pred,\n",
    "                    labels=labels,\n",
    "                    average=\"weighted\",\n",
    "                    zero_division=0\n",
    "                )\n",
    "\n",
    "        wf1_means.append(wf1)\n",
    "\n",
    "\n",
    "        # Normalize DoR and ORI\n",
    "        dor_values = subset[dor_col]\n",
    "        ori_values = subset[ori_col]\n",
    "        dor_normalized = (dor_values - dor_values.min()) / (dor_values.max() - dor_values.min()) if dor_values.max() != dor_values.min() else dor_values\n",
    "        ori_normalized = (ori_values - ori_values.min()) / (ori_values.max() - ori_values.min()) if ori_values.max() != ori_values.min() else ori_values\n",
    "\n",
    "        # Mean and variance of normalized DoR\n",
    "        dor_mean = dor_normalized.mean() if not subset.empty else 0\n",
    "        dor_var = dor_normalized.var() if not subset.empty else 0\n",
    "        dor_means.append(dor_mean)\n",
    "        dor_vars.append(dor_var)\n",
    "\n",
    "        # Mean and variance of normalized ORI\n",
    "        ori_mean = ori_normalized.mean() if not subset.empty else 0\n",
    "        ori_var = ori_normalized.var() if not subset.empty else 0\n",
    "        ori_means.append(ori_mean)\n",
    "        ori_vars.append(ori_var)\n",
    "\n",
    "        # Kendall τ for DoR\n",
    "        if len(subset) > 0 and dor_normalized.nunique() > 1:\n",
    "            tau_dor, _ = kendalltau(dor_normalized, subset[\"hit\"])\n",
    "        else:\n",
    "            tau_dor = 0\n",
    "        kendall_dor_taus.append(tau_dor)\n",
    "\n",
    "        # Kendall τ for ORI\n",
    "        if len(subset) > 0 and ori_normalized.nunique() > 1:\n",
    "            tau_ori, _ = kendalltau(ori_normalized, subset[\"hit\"])\n",
    "        else:\n",
    "            tau_ori = 0\n",
    "        kendall_ori_taus.append(tau_ori)\n",
    "\n",
    "    model_display_name = model_name_map[model_substring]\n",
    "\n",
    "    # Collect summary data\n",
    "    for prompt, wf1, dor_mean, dor_var, ori_mean, ori_var, tau_dor, tau_ori in zip(prompt_types, wf1_means, dor_means, dor_vars, ori_means, ori_vars, kendall_dor_taus, kendall_ori_taus):\n",
    "        summary_data.append({\n",
    "            'Model': model_display_name,\n",
    "            'Prompt Type': prompt,\n",
    "            'WF₁ Mean': wf1,\n",
    "            'Mean DoR': dor_mean,\n",
    "            'DoR Var.': dor_var,\n",
    "            'Mean Originality': ori_mean,\n",
    "            'Originality Var.': ori_var,\n",
    "            \"Kendall's τ (DoR)\": tau_dor,\n",
    "            \"Kendall's τ (ORI)\": tau_ori\n",
    "        })\n",
    "\n",
    "# Convert summary data to DataFrame\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Display summary table\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "\n",
    "# Prepare the data for PCA\n",
    "features = ['WF₁ Mean', 'Mean DoR', 'DoR Var.', 'Mean Originality', 'Originality Var.']\n",
    "x = summary_df[features]\n",
    "\n",
    "# Standardize the data\n",
    "x = (x - x.mean()) / x.std()\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(x)\n",
    "\n",
    "# Create a DataFrame with the PCA results\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2'])\n",
    "pca_df['Model'] = summary_df['Model']\n",
    "pca_df['Prompt Type'] = summary_df['Prompt Type']  # Assuming 'Prompt Type' is a column in summary_df\n",
    "\n",
    "# Calculate instability index for each model\n",
    "instability_scores = []\n",
    "for model in pca_df[\"Model\"].unique():\n",
    "    points = pca_df[pca_df[\"Model\"] == model][[\"Principal Component 1\", \"Principal Component 2\"]].values\n",
    "    # Calculate all distances between pairs of points\n",
    "    dists = []\n",
    "    for (i, j) in combinations(range(len(points)), 2):\n",
    "        dists.append(np.linalg.norm(points[i] - points[j]))\n",
    "    instab_index = np.mean(dists) / (len(points) * (len(points) - 1) / 2) if len(points) > 1 else 0\n",
    "    instability_scores.append({\n",
    "        \"Model\": model,\n",
    "        \"Instability Index\": instab_index\n",
    "    })\n",
    "\n",
    "# Set color palette\n",
    "palette = sns.color_palette(\"viridis\", len(pca_df['Prompt Type'].unique()))  # Changed to a more visually appealing palette\n",
    "\n",
    "# Create a figure for both PCA and dendrogram\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 6))  # Create a single figure with two subplots\n",
    "\n",
    "# Plot the PCA results\n",
    "sns.scatterplot(data=pca_df, x='Principal Component 1', y='Principal Component 2', hue='Prompt Type', palette=palette, alpha=0.8, s=100, edgecolor='w', linewidth=0.5, ax=axs[0])\n",
    "\n",
    "# Add lines to separate quadrants with a softer color\n",
    "axs[0].axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "axs[0].axvline(0, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "# Annotate points with model names, using a larger font size\n",
    "for i, model in enumerate(pca_df['Model']):\n",
    "    axs[0].annotate(model, (pca_df['Principal Component 1'][i], pca_df['Principal Component 2'][i]), fontsize=10, ha='right')\n",
    "\n",
    "# Add quadrant interpretations, centralized with a more elegant font style\n",
    "axs[0].text(1.5, 0.1, \"Compensated Instability Region\", fontsize=10, ha='center', fontweight='bold', color='darkorange')\n",
    "axs[0].text(-2, 0.1, \"Expressive Consistency Region\", fontsize=10, ha='center', fontweight='bold', color='darkorange')\n",
    "axs[0].text(-2, -0.2, \"Suppressed Reasoning Region\", fontsize=10, ha='center', fontweight='bold', color='darkorange')\n",
    "axs[0].text(1.5, -0.2, \"Misaligned Confidence Region\", fontsize=10, ha='center', fontweight='bold', color='darkorange')\n",
    "\n",
    "# Add explained variance for each dimension with a more prominent title\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "axs[0].set_title(f'PCA of Models Metrics\\nExplained Variance: PC1 = {explained_variance[0]:.2f}, PC2 = {explained_variance[1]:.2f}', fontsize=16)\n",
    "axs[0].set_xlabel('Principal Component 1', fontsize=12)\n",
    "axs[0].set_ylabel('Principal Component 2', fontsize=12)\n",
    "axs[0].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display loadings for each original criterion\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)  # Calculate loadings\n",
    "loadings_df = pd.DataFrame(loadings, index=features, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Display eigenvalues\n",
    "eigenvalues = pca.explained_variance_\n",
    "\n",
    "# Heterotrait-Monotrait (HTMT) ratio calculation\n",
    "def htmt_ratio(df_in, dor_cols, ori_cols, corr_method=\"spearman\"):\n",
    "    # Mantém só colunas relevantes e garante numérico\n",
    "    data = df_in[list(dor_cols) + list(ori_cols)].apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
    "\n",
    "    # Matriz de correlação (Spearman costuma ser mais robusto; pode usar \"pearson\" também)\n",
    "    corr = data.corr(method=corr_method)\n",
    "\n",
    "    # 1) Heterotrait-heteromethod: correlações entre indicadores de DoR e ORI\n",
    "    hetero = corr.loc[dor_cols, ori_cols].abs().to_numpy().ravel()\n",
    "    mean_hetero = hetero.mean()\n",
    "\n",
    "    # 2) Monotrait-heteromethod: correlações entre indicadores dentro de cada construto (sem diagonal)\n",
    "    dor_mtx = corr.loc[dor_cols, dor_cols].abs().to_numpy()\n",
    "    ori_mtx = corr.loc[ori_cols, ori_cols].abs().to_numpy()\n",
    "\n",
    "    dor_mono = dor_mtx[np.triu_indices_from(dor_mtx, k=1)]\n",
    "    ori_mono = ori_mtx[np.triu_indices_from(ori_mtx, k=1)]\n",
    "\n",
    "    mean_mono_dor = dor_mono.mean()\n",
    "    mean_mono_ori = ori_mono.mean()\n",
    "\n",
    "    denom = np.sqrt(mean_mono_dor * mean_mono_ori)\n",
    "    return float(mean_hetero / denom) if denom > 0 else np.nan\n",
    "\n",
    "# --- HTMT (DoR vs ORI) ---\n",
    "dor_cols = list(gr_dor_columns)   # indicadores: notas DoR de cada juiz\n",
    "ori_cols = list(gr_ori_columns)   # indicadores: notas ORI de cada juiz\n",
    "\n",
    "# Para reproduzir o “sob adversarial stress” mencionado no paper:\n",
    "df_htmt = df[df[\"prompt_type\"] == \"adversarial\"]\n",
    "\n",
    "htmt_score = htmt_ratio(df_htmt, dor_cols, ori_cols, corr_method=\"spearman\")\n",
    "\n",
    "# Dendrogram plot\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Perform hierarchical clustering using the Euclidean distance\n",
    "Z = linkage(pca_df[['Principal Component 1', 'Principal Component 2']], method='ward')\n",
    "\n",
    "# Create a dendrogram to visualize the clustering\n",
    "dendrogram(Z, labels=[f\"{model} ({prompt})\" for model, prompt in zip(pca_df['Model'].values, pca_df['Prompt Type'].values)], leaf_rotation=90, ax=axs[1])\n",
    "axs[1].set_title('Hierarchical Clustering Dendrogram', fontsize=16)\n",
    "axs[1].set_xlabel('', fontsize=12)\n",
    "axs[1].set_ylabel('Euclidean Distance', fontsize=12)  # Specify the distance used\n",
    "axs[1].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.savefig('figures/pca_and_dendrogram_plot.png', dpi=360, bbox_inches='tight')  # Save the combined figure\n",
    "plt.show()\n",
    "\n",
    "instability_df = pd.DataFrame(instability_scores).sort_values(by='Instability Index')\n",
    "\n",
    "# Print HTMT score for validity\n",
    "print(f\"Heterotrait-Monotrait (HTMT) score: {htmt_score:.4f}\")\n",
    "print(\"Loadings (coefficients) for each criterion:\")\n",
    "print(loadings_df)\n",
    "print(\"Eigenvalues:\")\n",
    "print(eigenvalues)\n",
    "print(instability_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Calculate effect sizes for DoR and ORI between hits and misses\n",
    "def calculate_effect_size(df, score_columns):\n",
    "    effect_sizes = {}\n",
    "    for col in score_columns:\n",
    "        hit_scores = df[df['hit'] == 1][col]\n",
    "        miss_scores = df[df['hit'] == 0][col]\n",
    "        # Calculate Cohen's d with the absolute difference\n",
    "        d = np.abs(hit_scores.mean() - miss_scores.mean()) / np.sqrt((hit_scores.std() ** 2 + miss_scores.std() ** 2) / 2)\n",
    "        # Convert d to r\n",
    "        r = d / np.sqrt(d**2 + 4)\n",
    "        effect_sizes[col] = r\n",
    "    return effect_sizes\n",
    "\n",
    "# Define score columns for DoR and ORI, ensuring both are included\n",
    "dor_cols = [col for col in df.columns if col.startswith('gr_dor_') and not col.endswith('median')]\n",
    "ori_cols = [col for col in df.columns if col.startswith('gr_ori_') and not col.endswith('median')]\n",
    "score_columns = dor_cols + ori_cols  # Include both DoR and ORI columns\n",
    "\n",
    "# Ensure df_hit0 is defined before calculating effect sizes\n",
    "df_hit0 = df.copy()  # Assuming df is defined and filtering for misses\n",
    "\n",
    "# Calculate effect sizes\n",
    "effect_sizes = calculate_effect_size(df_hit0, score_columns)\n",
    "\n",
    "# Print effect sizes\n",
    "print(\"Effect Sizes (r) for DoR and ORI between hits and misses:\")\n",
    "for col, size in effect_sizes.items():\n",
    "    print(f\"{col}: {size:.4f}\")\n",
    "\n",
    "# Test ACE on an expanded set of low-capacity models\n",
    "low_capacity_models = ['3-haiku', '4.1-nano', '4o-mini']  # Example low-capacity models\n",
    "expanded_df = df[df['model'].isin(low_capacity_models)]\n",
    "\n",
    "# Calculate ACE for the expanded set\n",
    "# Assuming ACE calculation function is defined elsewhere\n",
    "# ace_results = calculate_ace(expanded_df)\n",
    "\n",
    "# Print ACE results if applicable\n",
    "# print(\"ACE results for low-capacity models:\")\n",
    "# print(ace_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Qualitative outlier analysis for the hit = 0 cases in Deep of Reasoning and Originality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Filtro: apenas casos onde o modelo ERROU\n",
    "df_hit0 = df[df['hit'] == 0].copy()\n",
    "\n",
    "# Selecionar colunas com escores\n",
    "dor_cols = [col for col in df.columns if col.startswith('gr_dor_') and not col.endswith('median')]\n",
    "ori_cols = [col for col in df.columns if col.startswith('gr_ori_') and not col.endswith('median')]\n",
    "\n",
    "# Função para detectar outliers com base no IQR\n",
    "def detect_outliers_iqr(series):\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    return (series > upper)\n",
    "\n",
    "# Detectar outliers por modelo nas métricas DoR e ORI\n",
    "outlier_flags = pd.DataFrame(index=df_hit0.index)\n",
    "for col in dor_cols + ori_cols:\n",
    "    outlier_flags[col + '_outlier'] = detect_outliers_iqr(df_hit0[col])\n",
    "\n",
    "# Combinar os dados\n",
    "df_outliers = df_hit0[outlier_flags.any(axis=1)].copy()\n",
    "df_outliers['outlier_columns'] = outlier_flags.loc[df_outliers.index].apply(lambda row: [col for col, val in row.items() if val], axis=1)\n",
    "\n",
    "# Selecionar colunas úteis para análise\n",
    "relevant_cols = ['source', 'item', 'model', 'prompt_type', 'CoT'] + dor_cols + ori_cols + ['outlier_columns']\n",
    "df_outliers_view = df_outliers[relevant_cols]\n",
    "df_outliers_view = df_outliers_view.sort_values(by=['model', 'source', 'prompt_type'])\n",
    "\n",
    "df_outliers_view.to_csv('data/df_outliers_view.csv', index=False)\n",
    "len(df_outliers_view)\n",
    "\n",
    "# Definition of categories\n",
    "error_categories = {\n",
    "    \"Plausible but incorrect\": \"Coherent reasoning that ends in a wrong answer.\",\n",
    "    \"Concept confusion\": \"Misunderstanding of a core concept or principle.\",\n",
    "    \"Alternative interpretation\": \"Valid logic applied to an unconventional or unintended interpretation of the question.\",\n",
    "    \"Narrative drift\": \"Response veers into irrelevant or excessively verbose explanation.\",\n",
    "    \"Incomplete or truncated\": \"Reasoning is too brief or lacks a conclusion.\"\n",
    "}\n",
    "\n",
    "# Mock function for classification\n",
    "def classify_cot_mock(text):\n",
    "    text_lower = text.lower()\n",
    "    if \"definition\" in text_lower and \"but\" in text_lower and \"so\" in text_lower:\n",
    "        return \"Plausible but incorrect\"\n",
    "    elif \"confuse\" in text_lower or \"mistake\" in text_lower:\n",
    "        return \"Concept confusion\"\n",
    "    elif \"could mean\" in text_lower or \"might be interpreted\" in text_lower:\n",
    "        return \"Alternative interpretation\"\n",
    "    elif len(text_lower.split()) > 100:\n",
    "        return \"Narrative drift\"\n",
    "    elif len(text_lower.split()) < 10:\n",
    "        return \"Incomplete or truncated\"\n",
    "    else:\n",
    "        return \"Plausible but incorrect\"\n",
    "\n",
    "# Apply classification to all CoTs\n",
    "cot_classification_df = df_outliers_view.copy()\n",
    "cot_classification_df[\"CoT Category\"] = cot_classification_df[\"CoT\"].apply(classify_cot_mock)\n",
    "cot_classification_df.to_csv('data/cot_classification_df.csv', index=False)\n",
    "\n",
    "# Tabular result\n",
    "cot_classification_df[['model', 'prompt_type', 'CoT', 'CoT Category']]\n",
    "\n",
    "# Check the overall distribution of categories\n",
    "category_counts = cot_classification_df[\"CoT Category\"].value_counts().reset_index()\n",
    "category_counts.columns = [\"CoT Category\", \"Count\"]\n",
    "\n",
    "# Frequency by model and type of error\n",
    "model_category = cot_classification_df.groupby([\"model\", \"CoT Category\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Add row sums for model_category\n",
    "model_category.loc['Row Sum'] = model_category.sum()\n",
    "\n",
    "# Frequency by type of prompt\n",
    "prompt_category = cot_classification_df.groupby([\"prompt_type\", \"CoT Category\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Add row sums for prompt_category\n",
    "prompt_category.loc['Row Sum'] = prompt_category.sum()\n",
    "\n",
    "display(category_counts)\n",
    "display(model_category)\n",
    "display(prompt_category)\n",
    "\n",
    "# Count the examples by category\n",
    "category_counts = cot_classification_df[\"CoT Category\"].value_counts()\n",
    "\n",
    "# Select representative examples from each category\n",
    "examples = []\n",
    "\n",
    "for category in category_counts.index:\n",
    "    subset = cot_classification_df[cot_classification_df[\"CoT Category\"] == category]\n",
    "    # Choose the example with the longest CoT to best represent the category\n",
    "    representative = subset.loc[subset['CoT'].str.len().idxmax()]\n",
    "    examples.append({\n",
    "        \"Category\": category,\n",
    "        \"Model\": representative[\"model\"],\n",
    "        \"Prompt Type\": representative[\"prompt_type\"],\n",
    "        \"Question\": representative[\"item\"],\n",
    "        \"CoT\": representative[\"CoT\"]\n",
    "    })\n",
    "\n",
    "# Organize into DataFrame\n",
    "examples_df = pd.DataFrame(examples)\n",
    "display(examples_df)\n",
    "\n",
    "# Filtrar possíveis casos de desalinhamento moral\n",
    "alignment_relevant_categories = [\n",
    "    \"Plausible but incorrect\",\n",
    "    \"Alternative interpretation\",\n",
    "    \"Narrative drift\"\n",
    "]\n",
    "\n",
    "alignment_df = cot_classification_df[cot_classification_df[\"CoT Category\"].isin(alignment_relevant_categories)].copy()\n",
    "\n",
    "# Procurar menções a termos sensíveis que indicam dilemas morais, normas sociais, julgamentos\n",
    "keywords = [\n",
    "    \"should\", \"should not\", \"ethical\", \"moral\", \"deserve\", \"justice\", \"fair\", \"unfair\",\n",
    "    \"responsible\", \"dangerous\", \"appropriate\", \"inappropriate\", \"illegal\", \"respect\", \"harm\"\n",
    "]\n",
    "\n",
    "# Identificar CoTs que contêm essas palavras\n",
    "pattern = '|'.join(keywords)\n",
    "alignment_df[\"has_sensitive_terms\"] = alignment_df[\"CoT\"].str.lower().str.contains(pattern)\n",
    "\n",
    "# Selecionar os que possuem indícios linguísticos de avaliação moral\n",
    "morally_relevant = alignment_df[alignment_df[\"has_sensitive_terms\"] == True]\n",
    "display(morally_relevant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Qualitative outlier analysis for the hit = 1 cases in Deep of Reasoning and Originality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Filtro: apenas casos onde o modelo ERROU\n",
    "df_hit1 = df[df['hit'] == 1].copy()\n",
    "\n",
    "# Selecionar colunas com escores\n",
    "dor_cols = [col for col in df.columns if col.startswith('gr_dor_') and not col.endswith('median')]\n",
    "ori_cols = [col for col in df.columns if col.startswith('gr_ori_') and not col.endswith('median')]\n",
    "\n",
    "# Função para detectar outliers com base no IQR\n",
    "def detect_outliers_iqr(series):\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    return (series < lower)\n",
    "\n",
    "# Detectar outliers por modelo nas métricas DoR e ORI\n",
    "outlier_flags = pd.DataFrame(index=df_hit1.index)\n",
    "for col in dor_cols + ori_cols:\n",
    "    outlier_flags[col + '_outlier'] = detect_outliers_iqr(df_hit1[col])\n",
    "\n",
    "# Combinar os dados\n",
    "df_outliers = df_hit1[outlier_flags.any(axis=1)].copy()\n",
    "df_outliers['outlier_columns'] = outlier_flags.loc[df_outliers.index].apply(lambda row: [col for col, val in row.items() if val], axis=1)\n",
    "\n",
    "# Selecionar colunas úteis para análise\n",
    "relevant_cols = ['source', 'item', 'model', 'prompt_type', 'CoT'] + dor_cols + ori_cols + ['outlier_columns']\n",
    "df_outliers_view = df_outliers[relevant_cols]\n",
    "df_outliers_view = df_outliers_view.sort_values(by=['model', 'source', 'prompt_type'])\n",
    "\n",
    "df_outliers_view.to_csv('data/df_outliers_view_hit1.csv', index=False)\n",
    "len(df_outliers_view)\n",
    "\n",
    "# Definition of categories\n",
    "error_categories = {\n",
    "    \"Plausible but incorrect\": \"Coherent reasoning that ends in a wrong answer.\",\n",
    "    \"Concept confusion\": \"Misunderstanding of a core concept or principle.\",\n",
    "    \"Alternative interpretation\": \"Valid logic applied to an unconventional or unintended interpretation of the question.\",\n",
    "    \"Narrative drift\": \"Response veers into irrelevant or excessively verbose explanation.\",\n",
    "    \"Incomplete or truncated\": \"Reasoning is too brief or lacks a conclusion.\"\n",
    "}\n",
    "\n",
    "# Mock function for classification\n",
    "def classify_cot_mock(text):\n",
    "    text_lower = text.lower()\n",
    "    if \"definition\" in text_lower and \"but\" in text_lower and \"so\" in text_lower:\n",
    "        return \"Plausible but incorrect\"\n",
    "    elif \"confuse\" in text_lower or \"mistake\" in text_lower:\n",
    "        return \"Concept confusion\"\n",
    "    elif \"could mean\" in text_lower or \"might be interpreted\" in text_lower:\n",
    "        return \"Alternative interpretation\"\n",
    "    elif len(text_lower.split()) > 100:\n",
    "        return \"Narrative drift\"\n",
    "    elif len(text_lower.split()) < 10:\n",
    "        return \"Incomplete or truncated\"\n",
    "    else:\n",
    "        return \"Plausible but incorrect\"\n",
    "\n",
    "# Apply classification to all CoTs\n",
    "cot_classification_df = df_outliers_view.copy()\n",
    "cot_classification_df[\"CoT Category\"] = cot_classification_df[\"CoT\"].apply(classify_cot_mock)\n",
    "cot_classification_df.to_csv('data/cot_classification_df_hit1.csv', index=False)\n",
    "\n",
    "# Tabular result\n",
    "cot_classification_df[['model', 'prompt_type', 'CoT', 'CoT Category']]\n",
    "\n",
    "# Check the overall distribution of categories\n",
    "category_counts = cot_classification_df[\"CoT Category\"].value_counts().reset_index()\n",
    "category_counts.columns = [\"CoT Category\", \"Count\"]\n",
    "\n",
    "# Frequency by model and type of error\n",
    "model_category = cot_classification_df.groupby([\"model\", \"CoT Category\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Add row sums for model_category\n",
    "model_category.loc['Row Sum'] = model_category.sum()\n",
    "\n",
    "# Frequency by type of prompt\n",
    "prompt_category = cot_classification_df.groupby([\"prompt_type\", \"CoT Category\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Add row sums for prompt_category\n",
    "prompt_category.loc['Row Sum'] = prompt_category.sum()\n",
    "\n",
    "display(category_counts)\n",
    "display(model_category)\n",
    "display(prompt_category)\n",
    "\n",
    "# Count the examples by category\n",
    "category_counts = cot_classification_df[\"CoT Category\"].value_counts()\n",
    "\n",
    "# Select representative examples from each category\n",
    "examples = []\n",
    "\n",
    "for category in category_counts.index:\n",
    "    subset = cot_classification_df[cot_classification_df[\"CoT Category\"] == category]\n",
    "    # Choose the example with the longest CoT to best represent the category\n",
    "    representative = subset.loc[subset['CoT'].str.len().idxmax()]\n",
    "    examples.append({\n",
    "        \"Category\": category,\n",
    "        \"Model\": representative[\"model\"],\n",
    "        \"Prompt Type\": representative[\"prompt_type\"],\n",
    "        \"Question\": representative[\"item\"],\n",
    "        \"CoT\": representative[\"CoT\"]\n",
    "    })\n",
    "\n",
    "# Organize into DataFrame\n",
    "examples_df = pd.DataFrame(examples)\n",
    "display(examples_df)\n",
    "\n",
    "# Filtrar possíveis casos de desalinhamento moral\n",
    "alignment_relevant_categories = [\n",
    "    \"Plausible but incorrect\",\n",
    "    \"Alternative interpretation\",\n",
    "    \"Narrative drift\"\n",
    "]\n",
    "\n",
    "alignment_df = cot_classification_df[cot_classification_df[\"CoT Category\"].isin(alignment_relevant_categories)].copy()\n",
    "\n",
    "# Procurar menções a termos sensíveis que indicam dilemas morais, normas sociais, julgamentos\n",
    "keywords = [\n",
    "    \"should\", \"should not\", \"ethical\", \"moral\", \"deserve\", \"justice\", \"fair\", \"unfair\",\n",
    "    \"responsible\", \"dangerous\", \"appropriate\", \"inappropriate\", \"illegal\", \"respect\", \"harm\"\n",
    "]\n",
    "\n",
    "# Identificar CoTs que contêm essas palavras\n",
    "pattern = '|'.join(keywords)\n",
    "alignment_df[\"has_sensitive_terms\"] = alignment_df[\"CoT\"].str.lower().str.contains(pattern)\n",
    "\n",
    "# Selecionar os que possuem indícios linguísticos de avaliação moral\n",
    "morally_relevant = alignment_df[alignment_df[\"has_sensitive_terms\"] == True]\n",
    "display(morally_relevant)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
